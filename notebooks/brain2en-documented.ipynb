{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "### Parse custom arguments\n",
    "''' Explanation of the Arguments:\n",
    "model (string): Type of model to run from among (PITOM, ConvNet, MeNTALmini, MeNTAL\n",
    "subject (list of strings): subject id's as a list\n",
    "shift (integer): Amount by which the onset should be shifted\n",
    "lr (float): learning rate\n",
    "gpus (int): number of gpus for the model to run on\n",
    "epochs (int):\n",
    "batch-size (int):\n",
    "window-size (int): window size to consider for the word in ms\n",
    "bin-size (int): bin size in ms\n",
    "init-model (string): \n",
    "'''\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', type=str, default='MeNTAL')\n",
    "parser.add_argument('--subjects', nargs='*', default=['625', '676'])\n",
    "parser.add_argument('--shift', type=int, default=0)\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--gpus', type=int, default=16)\n",
    "parser.add_argument('--epochs', type=int, default=60)\n",
    "parser.add_argument('--batch-size', type=int, default=48)\n",
    "parser.add_argument('--window-size', type=int, default=2000)\n",
    "parser.add_argument('--bin-size', type=int, default=50)\n",
    "parser.add_argument('--init-model', type=str, default=None)\n",
    "parser.add_argument('--no-plot', action='store_false', default=False)\n",
    "parser.add_argument('--electrodes', nargs='*', default=list(range(1,65)))\n",
    "parser.add_argument('--vocab-min-freq', type=int, default=10)\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--shuffle', action=\"store_true\", default=False)\n",
    "parser.add_argument('--no-eval', action=\"store_true\", default=False)\n",
    "parser.add_argument('--temp', type=float, default=0.995)\n",
    "parser.add_argument('--tf-dmodel', type=int, default=64)\n",
    "parser.add_argument('--tf-dff', type=int, default=256)\n",
    "parser.add_argument('--tf-nhead', type=int, default=8)\n",
    "parser.add_argument('--tf-nlayer', type=int, default=12)\n",
    "parser.add_argument('--tf-dropout', type=float, default=0.05)\n",
    "parser.add_argument('--weight-decay', type=float, default=0.35)\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDict(*args):\n",
    "     return dict(((k, eval(k)) for k in args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Default Configuration\n",
    "'''\n",
    "exclude_words_class: words to be excluded from the classifier vocabulary\n",
    "exclude_words: words to be excluded from the tranformer vocabulary\n",
    "log_interval: \n",
    "'''\n",
    "CONFIG = {\n",
    "    \"begin_token\": \"<s>\",\n",
    "    \"datum_suffix\": (\"conversation_trimmed\", \"trimmed\"),\n",
    "    \"electrodes\": 64,\n",
    "    \"end_token\": \"</s>\",\n",
    "    \"exclude_words_class\": [\"sp\", \"{lg}\", \"{ns}\", \"it\", \"a\", \"an\", \"and\", \"are\",\\\n",
    "                      \"as\", \"at\", \"be\", \"being\", \"by\", \"for\", \"from\", \"is\",\\\n",
    "                      \"of\", \"on\", \"that\", \"that's\", \"the\", \"there\", \"there's\",\\\n",
    "                      \"this\", \"to\", \"their\", \"them\", \"these\", \"he\", \"him\",\\\n",
    "                      \"his\", \"had\", \"have\", \"was\", \"were\", \"would\"],\n",
    "    \"exclude_words\": [\"sp\", \"{lg}\", \"{ns}\"],\n",
    "    \"log_interval\": 32,\n",
    "    \"main_dir\": \"/scratch/gpfs/hgazula/brain2en\",\n",
    "    \"data_dir\": \"/scratch/gpfs/hgazula\",\n",
    "    \"num_cpus\": 8,\n",
    "    \"oov_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"print_pad\": 120,\n",
    "    \"train_convs\": '-train-convs.txt',\n",
    "    \"valid_convs\": '-valid-convs.txt'\n",
    "}\n",
    "\n",
    "if len(args.subjects) == 1:\n",
    "    if args.subjects[0] == '625':\n",
    "        CONFIG[\"datum_suffix\"] = [CONFIG[\"datum_suffix\"][0]]\n",
    "    elif args.subjects[0] == '676':\n",
    "        CONFIG[\"datum_suffix\"] = [CONFIG[\"datum_suffix\"][1]]\n",
    "\n",
    "### Model objectives\n",
    "MODEL_OBJ = {\n",
    "    \"ConvNet10\": \"classifier\",\n",
    "    \"PITOM\": \"classifier\",\n",
    "    \"MeNTALmini\": \"classifier\",\n",
    "    \"MeNTAL\": \"seq2seq\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPUs\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "args.gpus = min(args.gpus, torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Configuration: \n",
      "[('batch_size', 48), ('begin_token', '<s>'), ('bin_size', 50), ('data_dir', '/scratch/gpfs/hgazula'), ('datum_suffix', ('conversation_trimmed', 'trimmed')), ('electrodes', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]), ('end_token', '</s>'), ('epochs', 60), ('exclude_words', ['sp', '{lg}', '{ns}']), ('exclude_words_class', ['sp', '{lg}', '{ns}', 'it', 'a', 'an', 'and', 'are', 'as', 'at', 'be', 'being', 'by', 'for', 'from', 'is', 'of', 'on', 'that', \"that's\", 'the', 'there', \"there's\", 'this', 'to', 'their', 'them', 'these', 'he', 'him', 'his', 'had', 'have', 'was', 'were', 'would']), ('gpus', 0), ('init_model', None), ('log_interval', 32), ('lr', 0.0001), ('main_dir', '/scratch/gpfs/hgazula/brain2en'), ('model', 'MeNTAL'), ('no_eval', False), ('no_plot', False), ('num_cpus', 8), ('oov_token', '<unk>'), ('pad_token', '<pad>'), ('print_pad', 120), ('seed', 1234), ('shift', 0), ('shuffle', False), ('subjects', ['625', '676']), ('temp', 0.995), ('tf_dff', 256), ('tf_dmodel', 64), ('tf_dropout', 0.05), ('tf_nhead', 8), ('tf_nlayer', 12), ('train_convs', '-train-convs.txt'), ('valid_convs', '-valid-convs.txt'), ('vocab_min_freq', 10), ('weight_decay', 0.35), ('window_size', 2000)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "CONFIG.update(vars(args))\n",
    "print(\"Script Configuration: \")\n",
    "print(sorted(CONFIG.items()))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Fix random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format directory logistics\n",
    "CONV_DIRS = [\n",
    "    CONFIG[\"data_dir\"] + '/%s-conversations/' % i for i in args.subjects\n",
    "]\n",
    "META_DIRS = [CONFIG[\"data_dir\"] + '/%s-metadata/' % i for i in args.subjects]\n",
    "SAVE_DIR = './ipynbresults/%s/' % (args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "args.model = args.model.split(\"_\")[0]\n",
    "classify = False if (args.model in MODEL_OBJ\n",
    "                     and MODEL_OBJ[args.model] == \"seq2seq\") else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Conversations is: 63\n",
      "Number of Conversations is: 13\n",
      "Number of Conversations is: 49\n",
      "Number of Conversations is: 24\n"
     ]
    }
   ],
   "source": [
    "from data_util import read_file\n",
    "\n",
    "# Conversation splits\n",
    "TRAIN_CONV, VALID_CONV = [], []\n",
    "for meta, subject in zip(META_DIRS, args.subjects):\n",
    "    TRAIN_CONV.append(\n",
    "        read_file(\"%s%s%s\" % (meta, subject, CONFIG[\"train_convs\"])))\n",
    "    VALID_CONV.append(\n",
    "        read_file(\"%s%s%s\" % (meta, subject, CONFIG[\"valid_convs\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dirs = CONV_DIRS\n",
    "subjects = args.subjects\n",
    "conversations = TRAIN_CONV\n",
    "algo='unigram'\n",
    "vocab_size=1000\n",
    "exclude_words=['sp', '{lg}', '{ns}']\n",
    "datum_suffix=CONFIG[\"datum_suffix\"]\n",
    "oov_tok=CONFIG[\"oov_token\"]\n",
    "begin_tok=CONFIG[\"begin_token\"]\n",
    "end_tok=CONFIG[\"end_token\"]\n",
    "pad_tok=CONFIG[\"pad_token\"]\n",
    "min_freq=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "# Conversations: 112\n",
      "Vocabulary size (min_freq=10): 1389\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import glob \n",
    "import pandas as pd\n",
    "# Generating vocaulary from reading the datums\n",
    "exclude_words = set(exclude_words)\n",
    "word2freq = Counter()\n",
    "columns = [\"word\", \"onset\", \"offset\", \"accuracy\", \"speaker\"]\n",
    "# files = [\n",
    "#     f for conv_dir, subject, ds in zip(conv_dirs, subjects, datum_suffix)\n",
    "#     for f in glob.glob(conv_dir + f'NY{subject}*/misc/*datum_{ds}.txt')\n",
    "# ]\n",
    "\n",
    "convs = [(conv_dir + conv_name, '/misc/*datum_%s.txt' % ds, idx)\\\n",
    "         for idx, (conv_dir, convs, ds) in enumerate(zip(conv_dirs, conversations, datum_suffix))\\\n",
    "         for conv_name in convs]\n",
    "\n",
    "print(len(convs))\n",
    "\n",
    "conv_count = 0\n",
    "for conversation, suffix, idx in convs:\n",
    "    \n",
    "    # Check if files exists, if it doesn't go to next\n",
    "    datum_fn = glob.glob(conversation + suffix)[0]\n",
    "    if not datum_fn:\n",
    "        print('File DNE: ', conversation + suffix)\n",
    "        continue\n",
    "        \n",
    "    conv_count += 1\n",
    "    with open(datum_fn, 'r') as fin:\n",
    "        lines = map(lambda x: x.split(), fin)\n",
    "        examples = map(lambda x: (\" \".join([z for y in x[0:-4] if (z := y.lower().strip().replace('\"', '')) not in exclude_words])), lines)\n",
    "        examples = filter(lambda x: len(x) > 0, examples)\n",
    "        examples = list(map(lambda x: x.split(), examples))\n",
    "    word2freq.update(word for example in examples for word in example)\n",
    "\n",
    "    \n",
    "if min_freq > 1:\n",
    "    word2freq = {\n",
    "        word: freq\n",
    "        for word, freq in word2freq.items() if freq >= min_freq\n",
    "    }\n",
    "vocab = sorted(word2freq.keys())\n",
    "n_classes = len(vocab)\n",
    "w2i = {word: i for i, word in enumerate(vocab)}\n",
    "i2w = {i: word for word, i in w2i.items()}\n",
    "print(\"# Conversations:\", conv_count)\n",
    "print(\"Vocabulary size (min_freq=%d): %d\" % (min_freq, len(word2freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dirs = CONV_DIRS\n",
    "subjects = args.subjects\n",
    "conversations = TRAIN_CONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "# Conversations: 155\n",
      "Vocabulary size (unigram): 1000\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Assign tokens\n",
    "oov_token = oov_tok\n",
    "begin_token = begin_tok\n",
    "end_token = end_tok\n",
    "pad_token = pad_tok\n",
    "\n",
    "# set of exclude words\n",
    "exclude_words = set(exclude_words)\n",
    "columns = [\"word\", \"onset\", \"offset\", \"accuracy\", \"speaker\"]\n",
    "convs = [(conv_dir + conv_name, '/misc/*datum_%s.txt' % ds, idx)\\\n",
    "         for idx, (conv_dir, convs, ds) in enumerate(zip(conv_dirs, conversations, datum_suffix))\\\n",
    "         for conv_name in convs]\n",
    "\n",
    "print(len(convs))\n",
    "\n",
    "words = []\n",
    "\n",
    "conv_count = 0\n",
    "for conversation, suffix, idx in convs:\n",
    "    \n",
    "    # Check if files exists, if it doesn't go to next\n",
    "    datum_fn = glob.glob(conversation + suffix)[0]\n",
    "    if not datum_fn:\n",
    "        print('File DNE: ', conversation + suffix)\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(datum_fn,\n",
    "                     delimiter=' ',\n",
    "                     header=None,\n",
    "                     names=columns)\n",
    "    df.word = df.word.str.lower()\n",
    "    df = df[df.speaker == \"Speaker1\"]\n",
    "    df.word = df[~df.word.str.lower().isin(exclude_words)]\n",
    "    words.append(df.word.dropna().tolist())\n",
    "    \n",
    "wordsl = [item for sublist in words for item in sublist]\n",
    "with open(\"vocab_temp.txt\", \"w\") as fh:\n",
    "    fh.writelines(\"%s\\n\" % str(place) for place in wordsl)\n",
    "\n",
    "spm.SentencePieceTrainer.Train('--input=vocab_temp.txt --model_prefix=BrainTransformer --model_type=%s --vocab_size=%d --bos_id=0 --eos_id=1 --unk_id=2 --unk_surface=%s --pad_id=3' % (algo, vocab_size, oov_token))\n",
    "\n",
    "sys.stdout.flush()\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(\"BrainTransformer.model\")\n",
    "\n",
    "print(\"# Conversations:\", len(files))\n",
    "print(\"Vocabulary size (%s): %d\" % (algo, vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '<s>'), (1, '</s>'), (2, '<unk>'), (3, '<pad>'), (4, '▁i'), (5, \"'\"), (6, 's'), (7, '▁like'), (8, '▁you'), (9, 't'), (10, '▁yeah'), (11, '▁it'), (12, '▁a'), (13, '▁the'), (14, '▁to'), (15, '▁and'), (16, '▁know'), (17, '▁that'), (18, 'm'), (19, '▁so'), (20, 'ing'), (21, '▁just'), (22, 'n'), (23, '▁was'), (24, '▁they'), (25, '▁'), (26, 'y'), (27, '▁my'), (28, '▁me'), (29, 'e'), (30, '▁don'), (31, '▁of'), (32, 're'), (33, '▁in'), (34, 'ed'), (35, '▁but'), (36, '▁what'), (37, '▁have'), (38, '▁she'), (39, '▁no'), (40, 'd'), (41, 'ay'), (42, '▁be'), (43, '▁for'), (44, '▁um'), (45, '▁this'), (46, '▁oh'), (47, '▁on'), (48, '▁ok'), (49, '▁do'), (50, '▁not'), (51, 'a'), (52, '▁he'), (53, 'll'), (54, '▁go'), (55, '▁m'), (56, '▁right'), (57, '▁if'), (58, '▁did'), (59, '▁we'), (60, '▁there'), (61, '▁with'), (62, '▁can'), (63, '▁gonna'), (64, '▁good'), (65, 've'), (66, 'h'), (67, 'o'), (68, 'ly'), (69, '▁really'), (70, 'er'), (71, '▁one'), (72, 'r'), (73, '▁or'), (74, 'u'), (75, '▁when'), (76, '▁s'), (77, '▁is'), (78, '▁all'), (79, '▁feel'), (80, 'i'), (81, 'p'), (82, '▁your'), (83, '▁think'), (84, '▁now'), (85, '▁mean'), (86, '▁how'), (87, '▁get'), (88, '▁out'), (89, '▁are'), (90, '▁little'), (91, '▁would'), (92, '▁c'), (93, '▁had'), (94, '▁say'), (95, '▁her'), (96, '▁see'), (97, '▁then'), (98, 'hm'), (99, '▁hi'), (100, 'w'), (101, '▁uh'), (102, '▁p'), (103, 'c'), (104, 'le'), (105, '▁here'), (106, '▁up'), (107, '▁who'), (108, '▁wanna'), (109, '▁time'), (110, '▁could'), (111, '▁something'), (112, '▁about'), (113, '▁cause'), (114, '▁tell'), (115, '▁from'), (116, '▁two'), (117, '▁thank'), (118, '▁some'), (119, '▁at'), (120, '▁thing'), (121, 'at'), (122, 'g'), (123, '▁because'), (124, '▁people'), (125, '▁n'), (126, '▁were'), (127, 'l'), (128, 'in'), (129, 'al'), (130, '▁day'), (131, 'k'), (132, 'ld'), (133, '▁want'), (134, '▁al'), (135, '▁alright'), (136, '▁over'), (137, '▁need'), (138, '▁look'), (139, '▁kinda'), (140, '▁more'), (141, '▁yes'), (142, 'b'), (143, '▁does'), (144, '▁where'), (145, '▁b'), (146, '▁going'), (147, '▁re'), (148, 'ar'), (149, '▁them'), (150, '▁said'), (151, '▁got'), (152, '▁prob'), (153, 'abl'), (154, '▁exactly'), (155, '▁maybe'), (156, '▁why'), (157, '▁whatever'), (158, '▁even'), (159, '▁these'), (160, '▁take'), (161, 'ce'), (162, 'uch'), (163, '▁make'), (164, '▁three'), (165, '▁well'), (166, 'ever'), (167, '▁work'), (168, '▁home'), (169, '▁love'), (170, '▁any'), (171, 'en'), (172, '▁been'), (173, '▁pain'), (174, '▁come'), (175, '▁give'), (176, '▁f'), (177, 'on'), (178, 'or'), (179, '▁too'), (180, '▁bit'), (181, '▁lot'), (182, '▁an'), (183, '▁seizure'), (184, 'very'), (185, '▁hu'), (186, '▁might'), (187, 'te'), (188, '▁fi'), (189, '▁try'), (190, '▁him'), (191, '▁mom'), (192, '▁let'), (193, '▁doing'), (194, '▁their'), (195, '▁eat'), (196, '▁year'), (197, '▁us'), (198, 'ch'), (199, '▁back'), (200, '▁things'), (201, '▁other'), (202, '▁head'), (203, '▁anything'), (204, '▁co'), (205, 'ame'), (206, 'ent'), (207, 'se'), (208, 'tter'), (209, '▁put'), (210, 'up'), (211, 'f'), (212, '▁wait'), (213, '▁happen'), (214, '▁guys'), (215, 'rry'), (216, '▁start'), (217, '▁than'), (218, '▁before'), (219, '▁should'), (220, '▁help'), (221, '▁aw'), (222, '▁nice'), (223, 'il'), (224, 'th'), (225, '▁brain'), (226, '▁still'), (227, 'way'), (228, '▁course'), (229, '▁left'), (230, '▁fir'), (231, '▁has'), (232, '▁down'), (233, 'ic'), (234, '▁last'), (235, '▁bu'), (236, '▁sure'), (237, '▁by'), (238, '▁long'), (239, '▁everything'), (240, '▁t'), (241, 'ther'), (242, '▁today'), (243, '▁yea'), (244, '▁great'), (245, '▁those'), (246, '▁st'), (247, '▁place'), (248, '▁trying'), (249, 'es'), (250, '▁man'), (251, '▁four'), (252, '▁guy'), (253, '▁different'), (254, 'z'), (255, '▁after'), (256, '▁into'), (257, '▁will'), (258, 'an'), (259, '▁stuff'), (260, 'ies'), (261, 'ally'), (262, '▁o'), (263, '▁thought'), (264, '▁old'), (265, '▁hm'), (266, 'ra'), (267, '▁keep'), (268, '▁listen'), (269, '▁bad'), (270, '▁food'), (271, '▁ask'), (272, 'v'), (273, '▁hundred'), (274, '▁nothing'), (275, '▁though'), (276, '▁ma'), (277, '▁de'), (278, '▁hour'), (279, 'ne'), (280, '▁funn'), (281, '▁life'), (282, '▁watch'), (283, '▁way'), (284, '▁hear'), (285, 'ble'), (286, 'ope'), (287, '▁call'), (288, 'self'), (289, '▁hurt'), (290, '▁com'), (291, 'ck'), (292, '▁y'), (293, '▁wo'), (294, 'member'), (295, '▁fin'), (296, 'it'), (297, 'ion'), (298, 'id'), (299, 'ood'), (300, '▁medicine'), (301, 'round'), (302, 'um'), (303, '▁hard'), (304, '▁part'), (305, '▁every'), (306, '▁read'), (307, '▁school'), (308, 'ge'), (309, '▁off'), (310, '▁en'), (311, 'able'), (312, '▁god'), (313, '▁water'), (314, '▁prett'), (315, '▁actually'), (316, '▁ten'), (317, '▁un'), (318, '▁weird'), (319, '▁six'), (320, '▁guess'), (321, 'sh'), (322, '▁came'), (323, '▁night'), (324, 'read'), (325, 'ation'), (326, 'ine'), (327, '▁da'), (328, '▁sa'), (329, '▁fe'), (330, '▁care'), (331, '▁hell'), (332, '▁ever'), (333, '▁w'), (334, '▁ye'), (335, '▁talk'), (336, 'lso'), (337, 'ri'), (338, '▁e'), (339, '▁d'), (340, '▁tomorrow'), (341, '▁twenty'), (342, '▁open'), (343, '▁sleep'), (344, 'ow'), (345, '▁took'), (346, 'went'), (347, '▁lo'), (348, '▁side'), (349, '▁ab'), (350, 'age'), (351, '▁car'), (352, '▁drink'), (353, '▁ga'), (354, '▁stay'), (355, '▁eight'), (356, '▁getting'), (357, '▁thro'), (358, '▁second'), (359, 'ool'), (360, '▁tru'), (361, 'ment'), (362, '▁mind'), (363, '▁ha'), (364, '▁u'), (365, 'ke'), (366, 'ity'), (367, '▁move'), (368, '▁patient'), (369, '▁close'), (370, '▁honest'), (371, '▁dollar'), (372, '▁j'), (373, '▁live'), (374, '▁wanted'), (375, '▁bl'), (376, '▁use'), (377, 'co'), (378, 'side'), (379, '▁family'), (380, '▁house'), (381, '▁sound'), (382, '▁ear'), (383, '▁seven'), (384, 'plane'), (385, 'ugh'), (386, '▁leave'), (387, 'uh'), (388, 'ntil'), (389, '▁li'), (390, 'x'), (391, '▁thirty'), (392, '▁r'), (393, 'other'), (394, '▁bed'), (395, '▁believe'), (396, '▁walk'), (397, '▁having'), (398, '▁nurs'), (399, '▁again'), (400, '▁face'), (401, 'ful'), (402, '▁play'), (403, 'us'), (404, '▁pee'), (405, '▁ja'), (406, 'ct'), (407, '▁morning'), (408, '▁ah'), (409, '▁wor'), (410, '▁doctor'), (411, '▁minute'), (412, '▁hand'), (413, '▁pr'), (414, '▁under'), (415, '▁interest'), (416, '▁attenti'), (417, '▁air'), (418, 'ist'), (419, 'ice'), (420, '▁talking'), (421, '▁birthday'), (422, '▁sometime'), (423, '▁dad'), (424, '▁young'), (425, '▁haven'), (426, '▁nurse'), (427, '▁am'), (428, 'body'), (429, 'ough'), (430, 'uestion'), (431, 'go'), (432, 'ext'), (433, 'om'), (434, '▁girl'), (435, '▁hospital'), (436, '▁thousand'), (437, '▁yesterday'), (438, '▁pull'), (439, '▁happy'), (440, '▁real'), (441, '▁name'), (442, 'ake'), (443, '▁su'), (444, '▁la'), (445, 'ely'), (446, 'day'), (447, 'ey'), (448, '▁bathroom'), (449, '▁big'), (450, '▁gotta'), (451, '▁being'), (452, 'el'), (453, 'est'), (454, '▁mad'), (455, '▁looking'), (456, '▁medication'), (457, '▁understand'), (458, '▁show'), (459, '▁problem'), (460, '▁done'), (461, '▁mon'), (462, 'one'), (463, '▁goes'), (464, 'ive'), (465, '▁kind'), (466, '▁fa'), (467, 'per'), (468, '▁pleas'), (469, '▁cra'), (470, '▁el'), (471, '▁both'), (472, '▁nutrient'), (473, '▁job'), (474, '▁nine'), (475, 'ture'), (476, '▁age'), (477, 'some'), (478, 'ite'), (479, '▁bac'), (480, '▁vo'), (481, 'ight'), (482, '▁dog'), (483, '▁whi'), (484, '▁studie'), (485, '▁surgery'), (486, '▁light'), (487, '▁dr'), (488, '▁bar'), (489, '▁kid'), (490, 'ute'), (491, 'ul'), (492, '▁mm'), (493, '▁ro'), (494, '▁dis'), (495, '▁micro'), (496, '▁boy'), (497, '▁week'), (498, '▁sens'), (499, 'ui'), (500, 'min'), (501, '▁fel'), (502, 'ie'), (503, 'sc'), (504, '▁ensure'), (505, '▁clear'), (506, '▁new'), (507, '▁own'), (508, '▁vita'), (509, '▁cere'), (510, '▁somebody'), (511, 'ize'), (512, 'ant'), (513, 'vel'), (514, 'most'), (515, '▁ra'), (516, '▁gra'), (517, '▁scienti'), (518, '▁turt'), (519, '▁umbrella'), (520, '▁half'), (521, '▁amaz'), (522, '▁bab'), (523, '▁para'), (524, 'teria'), (525, '▁game'), (526, '▁finish'), (527, '▁everybody'), (528, '▁which'), (529, 'atch'), (530, 'op'), (531, '▁lay'), (532, 'ick'), (533, '▁top'), (534, '▁definitely'), (535, '▁jump'), (536, '▁phone'), (537, '▁leg'), (538, '▁month'), (539, '▁our'), (540, 'unch'), (541, '▁seen'), (542, '▁far'), (543, 'ry'), (544, 'ng'), (545, '▁person'), (546, '▁computer'), (547, '▁focus'), (548, '▁perfect'), (549, '▁leas'), (550, '▁idea'), (551, 'ance'), (552, 'ple'), (553, '▁cut'), (554, '▁group'), (555, '▁professor'), (556, '▁viol'), (557, '▁turn'), (558, 'tifi'), (559, '▁pass'), (560, '▁sitting'), (561, '▁room'), (562, '▁mou'), (563, '▁find'), (564, '▁heard'), (565, 'ue'), (566, 'lock'), (567, '▁stop'), (568, '▁someone'), (569, 'less'), (570, '▁sit'), (571, '▁most'), (572, '▁ho'), (573, '▁smell'), (574, '▁knew'), (575, 'solut'), (576, 'olog'), (577, '▁making'), (578, '▁sweet'), (579, '▁later'), (580, '▁hold'), (581, '▁matter'), (582, 'night'), (583, '▁spe'), (584, 'late'), (585, 'appoint'), (586, '▁popp'), (587, '▁stand'), (588, '▁fift'), (589, 'urni'), (590, 'fi'), (591, '▁ta'), (592, 'ip'), (593, '▁tea'), (594, '▁headache'), (595, '▁wrong'), (596, '▁fifteen'), (597, 'twe'), (598, 'ous'), (599, 'li'), (600, '▁eyes'), (601, '▁mov'), (602, '▁hope'), (603, '▁glad'), (604, '▁pressure'), (605, '▁psycholog'), (606, '▁couple'), (607, '▁sister'), (608, '▁taking'), (609, '▁scar'), (610, '▁suck'), (611, '▁forty'), (612, '▁lady'), (613, '▁hot'), (614, '▁document'), (615, '▁husband'), (616, '▁percent'), (617, '▁basic'), (618, '▁bring'), (619, '▁underst'), (620, '▁ob'), (621, '▁stor'), (622, 'telli'), (623, 'ince'), (624, '▁less'), (625, '▁word'), (626, 'muse'), (627, '▁kids'), (628, 'ence'), (629, '▁ti'), (630, '▁beauti'), (631, '▁college'), (632, '▁answer'), (633, '▁serious'), (634, '▁kept'), (635, '▁spend'), (636, '▁figure'), (637, 'vious'), (638, '▁change'), (639, '▁pick'), (640, '▁song'), (641, 'oon'), (642, '▁buy'), (643, 'ode'), (644, '▁floor'), (645, '▁particular'), (646, '▁point'), (647, '▁high'), (648, '▁normal'), (649, '▁spa'), (650, '▁throw'), (651, '▁friend'), (652, 'teen'), (653, '▁nobody'), (654, '▁exp'), (655, '▁such'), (656, '▁ex'), (657, 'ish'), (658, '▁fun'), (659, '▁therap'), (660, '▁confus'), (661, '▁epileps'), (662, '▁miss'), (663, '▁fiance'), (664, '▁trie'), (665, '▁arm'), (666, '▁ball'), (667, '▁pl'), (668, 'ech'), (669, 'friend'), (670, 'more'), (671, '▁ke'), (672, '▁giving'), (673, '▁sign'), (674, '▁ei'), (675, '▁wak'), (676, '▁worr'), (677, 'wis'), (678, 'der'), (679, 'out'), (680, 'ian'), (681, '▁operation'), (682, '▁reason'), (683, 'book'), (684, '▁order'), (685, '▁crap'), (686, '▁jo'), (687, '▁th'), (688, '▁won'), (689, '▁pot'), (690, 'uck'), (691, '▁imag'), (692, '▁yo'), (693, '▁class'), (694, '▁depend'), (695, '▁surprise'), (696, '▁prote'), (697, '▁apple'), (698, '▁hotel'), (699, '▁drive'), (700, '▁text'), (701, '▁tra'), (702, '▁lad'), (703, '▁shi'), (704, '▁poo'), (705, 'work'), (706, 'where'), (707, '▁win'), (708, '▁check'), (709, '▁nause'), (710, '▁promise'), (711, '▁supposed'), (712, '▁dream'), (713, '▁foot'), (714, '▁breath'), (715, '▁brought'), (716, '▁smart'), (717, '▁luck'), (718, 'ensive'), (719, '▁hopeful'), (720, '▁wish'), (721, '▁va'), (722, '▁din'), (723, '▁bag'), (724, 'our'), (725, '▁bother'), (726, '▁eye'), (727, 'ward'), (728, '▁end'), (729, '▁di'), (730, 'ball'), (731, '▁gu'), (732, '▁glass'), (733, '▁research'), (734, '▁restaurant'), (735, '▁grandm'), (736, '▁learn'), (737, '▁uncle'), (738, '▁each'), (739, '▁twel'), (740, '▁physical'), (741, '▁hair'), (742, '▁field'), (743, 'ertain'), (744, '▁therapist'), (745, '▁laugh'), (746, '▁everyone'), (747, 'vision'), (748, '▁bro'), (749, '▁comfort'), (750, 'loud'), (751, '▁till'), (752, '▁fall'), (753, '▁chan'), (754, 'liter'), (755, '▁wi'), (756, 'ppy'), (757, 'fast'), (758, '▁article'), (759, '▁front'), (760, '▁america'), (761, '▁study'), (762, '▁type'), (763, '▁slow'), (764, '▁exam'), (765, '▁fla'), (766, '▁writ'), (767, 'eeth'), (768, '▁finger'), (769, 'parent'), (770, '▁test'), (771, 'rta'), (772, 'nger'), (773, '▁mother'), (774, 'back'), (775, 'league'), (776, '▁charge'), (777, '▁cheese'), (778, '▁effect'), (779, '▁small'), (780, '▁terri'), (781, '▁cousin'), (782, '▁remind'), (783, '▁proce'), (784, '▁season'), (785, '▁stress'), (786, '▁dur'), (787, '▁elev'), (788, '▁eas'), (789, 'ude'), (790, '▁fuck'), (791, '▁deep'), (792, '▁tv'), (793, 'lier'), (794, 'mount'), (795, '▁movie'), (796, '▁extr'), (797, 'ious'), (798, 'entia'), (799, 'ngu'), (800, '▁total'), (801, '▁liv'), (802, '▁thir'), (803, '▁decide'), (804, '▁graduate'), (805, '▁random'), (806, '▁russian'), (807, '▁strong'), (808, '▁window'), (809, '▁speak'), (810, '▁music'), (811, '▁garb'), (812, '▁lobe'), (813, '▁blah'), (814, '▁human'), (815, '▁diet'), (816, '▁bottle'), (817, 'ection'), (818, '▁fact'), (819, '▁chic'), (820, 'j'), (821, '▁cost'), (822, 'jo'), (823, '▁break'), (824, '▁follow'), (825, '▁pound'), (826, '▁spoke'), (827, '▁switch'), (828, '▁tempora'), (829, '▁short'), (830, '▁agree'), (831, '▁clean'), (832, 'ccup'), (833, '▁cook'), (834, '▁send'), (835, '▁wonder'), (836, '▁calm'), (837, '▁forget'), (838, 'izz'), (839, '▁taste'), (840, '▁stick'), (841, '▁kn'), (842, '▁sub'), (843, '▁stra'), (844, 'ative'), (845, '▁usual'), (846, '▁sug'), (847, 'nside'), (848, 'ada'), (849, 'hol'), (850, '▁themsel'), (851, '▁vi'), (852, '▁kitchen'), (853, '▁number'), (854, '▁review'), (855, '▁ridiculous'), (856, '▁turk'), (857, '▁screw'), (858, '▁expect'), (859, '▁chill'), (860, '▁crow'), (861, '▁level'), (862, '▁health'), (863, '▁message'), (864, '▁jew'), (865, '▁step'), (866, '▁pill'), (867, '▁grandpa'), (868, '▁free'), (869, 'cept'), (870, '▁mommy'), (871, '▁full'), (872, '▁teach'), (873, '▁spi'), (874, '▁electr'), (875, '▁grow'), (876, '▁relax'), (877, '▁cigar'), (878, '▁driving'), (879, '▁inform'), (880, '▁activit'), (881, '▁complete'), (882, '▁purp'), (883, '▁lower'), (884, '▁children'), (885, '▁phil'), (886, '▁grape'), (887, '▁perco'), (888, 'tuna'), (889, 'uiet'), (890, '▁gy'), (891, '▁price'), (892, '▁cho'), (893, '▁wif'), (894, 'aving'), (895, 'meal'), (896, 'nol'), (897, 'come'), (898, '▁child'), (899, '▁banana'), (900, '▁calorie'), (901, '▁continue'), (902, '▁excuse'), (903, '▁credit'), (904, '▁favor'), (905, '▁annoy'), (906, '▁box'), (907, '▁fair'), (908, '▁assistan'), (909, '▁visit'), (910, '▁kick'), (911, '▁radi'), (912, '▁women'), (913, '▁run'), (914, '▁mush'), (915, '▁raise'), (916, '▁assum'), (917, '▁phd'), (918, '▁forgot'), (919, '▁bru'), (920, '▁pai'), (921, '▁twi'), (922, '▁sea'), (923, '▁tingl'), (924, '▁fur'), (925, 'comfort'), (926, 'room'), (927, 'sleep'), (928, '▁hyperventilat'), (929, '▁affect'), (930, '▁diagnose'), (931, '▁draft'), (932, '▁photo'), (933, '▁pretend'), (934, '▁stabb'), (935, '▁stomach'), (936, '▁giant'), (937, '▁picture'), (938, '▁coff'), (939, '▁certifi'), (940, '▁clot'), (941, '▁meditat'), (942, 'lphi'), (943, '▁brown'), (944, '▁accept'), (945, '▁boat'), (946, '▁wipe'), (947, 'cros'), (948, '▁personalit'), (949, '▁frid'), (950, 'hab'), (951, '▁sand'), (952, 'ife'), (953, '▁leav'), (954, '▁bott'), (955, '▁nervous'), (956, 'hydrate'), (957, 'partment'), (958, '▁appeti'), (959, '▁blanket'), (960, '▁confiden'), (961, '▁control'), (962, '▁cruise'), (963, '▁difficult'), (964, '▁emotion'), (965, '▁glove'), (966, '▁million'), (967, '▁staff'), (968, '▁support'), (969, '▁appro'), (970, '▁condo'), (971, '▁knock'), (972, '▁remove'), (973, '▁female'), (974, '▁smile'), (975, '▁impor'), (976, '▁marri'), (977, '▁sanit'), (978, '▁eighteen'), (979, '▁post'), (980, '▁score'), (981, '▁nap'), (982, '▁case'), (983, 'cream'), (984, 'apes'), (985, 'vide'), (986, 'ippin'), (987, 'uff'), (988, 'gur'), (989, '▁exact'), (990, '▁chair'), (991, '▁found'), (992, '▁marijuana'), (993, '▁necessar'), (994, '▁positiv'), (995, '▁swallow'), (996, '▁wheelchair'), (997, '▁especially'), (998, '▁fantas'), (999, '▁repeat')]\n"
     ]
    }
   ],
   "source": [
    "print([(i, vocab.IdToPiece(i)) for i in range(len(vocab))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================================\n",
    "\n",
    "==========================================================================================================\n",
    "\n",
    "==========================================================================================================\n",
    "\n",
    "=========================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get electrode date helper\n",
    "def get_electrode(elec_id):\n",
    "    conversation, electrode = elec_id\n",
    "    search_str = conversation + f'/preprocessed/*_{electrode}.mat'\n",
    "    mat_fn = glob.glob(search_str)\n",
    "    if len(mat_fn) == 0:\n",
    "        print(f'[WARNING] electrode {electrode} DNE in {search_str}')\n",
    "        return None\n",
    "    return loadmat(mat_fn[0])['p1st'].squeeze().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function input arguments\n",
    "conv_dirs = CONV_DIRS\n",
    "subjects = args.subjects\n",
    "conversations = TRAIN_CONV\n",
    "delimiter=\" \"\n",
    "bin_ms=args.bin_size\n",
    "shift_ms=args.shift\n",
    "window_ms=args.window_size\n",
    "electrodes=args.electrodes\n",
    "datum_suffix=CONFIG[\"datum_suffix\"]\n",
    "exclude_words=['sp', '{lg}', '{ns}']\n",
    "aug_shift_ms=[-1000, -500]\n",
    "fs = 512\n",
    "\n",
    "# extra stuff that happens inside\n",
    "oov_token=CONFIG[\"oov_token\"]\n",
    "begin_token=CONFIG[\"begin_token\"]\n",
    "end_token=CONFIG[\"end_token\"]\n",
    "pad_token=CONFIG[\"pad_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from multiprocessing import Pool\n",
    "from scipy.io import loadmat\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from matplotlib import rc\n",
    "# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "# ## for Palatino and other serif fonts use:\n",
    "# #rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "# rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals, labels = [], []\n",
    "bin_fs = int(bin_ms / 1000 * fs)\n",
    "shift_fs = int(shift_ms / 1000 * fs)\n",
    "window_fs = int(window_ms / 1000 * fs)\n",
    "half_window = window_fs // 2\n",
    "start_offset = - half_window + shift_fs\n",
    "end_offset = half_window + shift_fs\n",
    "aug_shift_fs = [int(s / 1000 * fs) for s in aug_shift_ms]\n",
    "\n",
    "signal_param_dict = createDict('bin_fs',\n",
    "                               'shift_fs',\n",
    "                               'window_fs',\n",
    "                               'half_window',\n",
    "                               'start_offset',\n",
    "                               'end_offset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = [(conv_dir + conv_name, '/misc/*datum_%s.txt' % ds, idx)\\\n",
    "         for idx, (conv_dir, convs, ds) in enumerate(zip(conv_dirs, conversations, datum_suffix))\\\n",
    "         for conv_name in convs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_electrode_array(conv, suf):\n",
    "    # Read signals\n",
    "    elec_ids = ((conversation, electrode) for electrode in electrodes)\n",
    "    with Pool() as pool:\n",
    "        ecogs = list(filter(lambda x: x is not None,\n",
    "                            pool.map(get_electrode, elec_ids)))\n",
    "\n",
    "    ecogs = np.asarray(ecogs)\n",
    "    ecogs = (ecogs - ecogs.mean(axis=1).reshape(ecogs.shape[0], 1)) / ecogs.std(axis=1).reshape(ecogs.shape[0], 1)\n",
    "    ecogs = ecogs.T\n",
    "    assert(ecogs.ndim == 2 and ecogs.shape[1] == len(electrodes))\n",
    "    return ecogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_w2i(string, wtoi, oov_tok):\n",
    "#     index_list = [wtoi[x] if x in wtoi else wtoi[oov_tok] for x in string.split(' ')]\n",
    "#     return index_list\n",
    "\n",
    "\n",
    "# def return_examples(file, delim, word2i, oov_tok, vocab):\n",
    "#     columns = [\"word\", \"onset\", \"offset\", \"accuracy\", \"speaker\"]  # columns in the datum\n",
    "#     df = pd.read_csv(file, delimiter=' ', header=None, names=columns)\n",
    "\n",
    "#     df.drop(columns=['offset', 'accuracy'], inplace=True)  # dropping offset and accuracy\n",
    "#     df.word = df.word.str.lower()  # converting words to lower case\n",
    "#     df = df[df.word != 'sp']  # deleting rows with 'sp' in word column\n",
    "#     df.sort_values(by=['onset'], inplace=True)  # sorting the datum based on onset\n",
    "#     df.speaker = df.speaker.str.strip() # stripping \\n from Speaker\n",
    "        \n",
    "# #     # concatenate strings where the onset is the same for the same speaker\n",
    "# #     df['word'] = df.groupby(['onset', 'speaker'])['word'].transform(lambda x: ' '.join(x))\n",
    "#     df.drop_duplicates()\n",
    "    \n",
    "#     '''# Example for the above line\n",
    "#     a = [['hello', 'Speaker 1', 6445],\n",
    "#          ['zaid', 'Speaker 1', 6445]\n",
    "#          ['harsha', 'Speaker 2', 6445],\n",
    "#          ['yeah', 'Speaker 1', 7345]\n",
    "#         ] \n",
    "#     df_a = pd.DataFrame(a, columns=['word', 'speaker', 'onset'])\n",
    "#     '''\n",
    "#     print(df.word.tolist()[:50])\n",
    "#     df.speaker = df.speaker == 'Speaker1'  # check if speaker 1\n",
    "#     df.word = df.word.apply(lambda x: generate_w2i(x, word2i, oov_tok))\n",
    "#     print(df.word.tolist()[:50])\n",
    "\n",
    "#     df = df[['word', 'speaker', 'onset']]\n",
    "#     print(df.head(50))\n",
    "#     df_vec = df.onset.diff() <= 0\n",
    "#     plt.figure()\n",
    "#     plt.plot(df.onset)\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.plot(df_vec[1:])\n",
    "  \n",
    "#     examples = list(df.to_records(index=False))\n",
    "    \n",
    "#     ####### part 2 ######\n",
    "\n",
    "        \n",
    "#     return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This following block of code extracts  bi-grams and their corresponding neural signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_examples_std(file, delim, vocabulary, ex_words):\n",
    "    with open(file, 'r') as fin:\n",
    "        lines = map(lambda x: x.split(delim), fin)\n",
    "        examples = map(lambda x: (\" \".join([z for y in x[0:-4] if (z := y.lower().strip().replace('\"', '')) not in ex_words]),\n",
    "                                  x[-1].strip() == \"Speaker1\",\n",
    "                                  x[-4], x[-3]), lines)\n",
    "        examples = filter(lambda x: len(x[0]) > 0, examples)\n",
    "        examples = map(lambda x: ([vocabulary[x] for x in x[0].split()], x[1], int(float(x[2])), int(float(x[3]))), examples)\n",
    "        return list(examples)\n",
    "    \n",
    "    \n",
    "def return_examples_spm(file, delim, vocabulary, ex_words):\n",
    "    \n",
    "    with open(file, 'r') as fin:\n",
    "        lines = map(lambda x: x.split(delim), fin)\n",
    "        examples = map(lambda x: (\" \".join([z for y in x[0:-4] if (z := y.lower().strip().replace('\"', '')) not in ex_words]),\n",
    "                                  x[-1].strip() == \"Speaker1\",\n",
    "                                  x[-4], x[-3]), lines)\n",
    "        examples = filter(lambda x: len(x[0]) > 0, examples)\n",
    "        examples = map(lambda x: (vocabulary.EncodeAsIds(x[0]), x[1], int(float(x[2])), int(float(x[3]))), examples)\n",
    "        return list(examples)\n",
    "\n",
    "    \n",
    "def generate_wordpairs_ghub(examples):\n",
    "    '''if the first set already has two words and is speaker 1\n",
    "        if the second set already has two words and is speaker 1\n",
    "        the onset of the first word is earlier than the second word\n",
    "    '''\n",
    "    my_grams = []\n",
    "    for first, second in zip(examples, examples[1:]):\n",
    "        len1, len2 = len(first[0]), len(second[0])\n",
    "        if first[1] and len1 == 2:\n",
    "            my_grams.append(first)\n",
    "        if second[1] and len2 == 2:\n",
    "            my_grams.append(second)\n",
    "        if ((first[1] and second[1]) and (len1 == 1 and len2 == 1)\n",
    "                and (first[2] < second[2])):\n",
    "            ak = (first[0] + second[0], True, first[2], second[3])\n",
    "            my_grams.append(ak)\n",
    "    return my_grams\n",
    "\n",
    "\n",
    "def generate_wordpairs(examples):\n",
    "    my_grams = []\n",
    "    for first, second in zip(examples, examples[1:]):\n",
    "        len1, len2 = len(first[0]), len(second[0])\n",
    "        if first[1] and len1 == 2:  # if the first set already has two words and is speaker 1\n",
    "            my_grams.append(first)\n",
    "        if second[1] and len2 == 2:  # if the second set already has two words and is speaker 1\n",
    "            my_grams.append(second)\n",
    "        if first[1] and second[1]:\n",
    "            if len1 == 1 and len2 == 1:\n",
    "                if first[2] < second[2]:  # the onset of the first word is earlier than the second word\n",
    "                    ak = (first[0] + second[0], True, first[2], second[3])\n",
    "                    my_grams.append(ak)\n",
    "    return my_grams\n",
    "\n",
    "\n",
    "def remove_duplicates(grams):\n",
    "    df = pd.DataFrame(grams)\n",
    "    df[['fw', 'sw']] = pd.DataFrame(df[0].tolist()) \n",
    "    df = df.drop(columns=[0]).drop_duplicates()\n",
    "    df[0] = df[['fw', 'sw']].values.tolist()\n",
    "    df = df.drop(columns=['fw', 'sw'])\n",
    "    df = df[sorted(df.columns)]\n",
    "    return list(df.to_records(index=False))\n",
    "\n",
    "\n",
    "def add_begin_end_tokens(word_pair, vocabulary, start_tok, stop_tok):\n",
    "    word_pair.insert(0, vocabulary[start_tok])  # Add start token\n",
    "    word_pair.append(vocabulary[stop_tok])  # Add end token\n",
    "    return word_pair\n",
    "\n",
    "\n",
    "def test_for_bad_window(start, stop, shape, window):\n",
    "    return (start < 0 or  # if the window_begin is less than 0 or\n",
    "        start > shape[0] or  # check if onset is within limits\n",
    "        stop < 0 or  # if the window_end is less than 0 or\n",
    "        stop > shape[0] or  # if the window_end is outside the signal \n",
    "        stop - start < window)  # if there are not enough frames in the window\n",
    "\n",
    "\n",
    "def calculate_windows_params(gram, param_dict):\n",
    "    seq_length = gram[3] - gram[2]\n",
    "    begin_window = bigram[2] + param_dict['start_offset']\n",
    "    end_window = bigram[3] + param_dict['end_offset']\n",
    "    bin_size = int(math.ceil((end_window - begin_window) / param_dict['bin_fs']))  # calculate number of bins\n",
    "\n",
    "    return seq_length, begin_window, end_window, bin_size\n",
    "\n",
    "\n",
    "def remove_oovs(grams, vocabulary, data_tag='train'):\n",
    "    if data_tag == 'train':\n",
    "        grams = filter(lambda x: vocabulary['<unk>'] not in x[0], grams)\n",
    "    else:\n",
    "        grams = filter(lambda x: x[0] != [vocabulary['<unk>']] * 2, grams)\n",
    "    return list(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final\n"
     ]
    }
   ],
   "source": [
    "train_seq_lengths, signals, labels = [], [], []\n",
    "for conversation, suffix, idx in convs:\n",
    "    \n",
    "    # Check if files exists, if it doesn't go to next\n",
    "    datum_fn = glob.glob(conversation + suffix)[0]\n",
    "    if not datum_fn:\n",
    "        print('File DNE: ', conversation + suffix)\n",
    "        continue\n",
    "        \n",
    "    # Extract electrode data\n",
    "    ecogs = return_electrode_array(conversation, suffix)\n",
    "    if not ecogs.size:\n",
    "        print(f'Skipping bad conversation: {conversation}')\n",
    "        continue\n",
    "\n",
    "    examples = return_examples_spm(datum_fn, delimiter, vocab, exclude_words)  # for spm\n",
    "    bigrams = generate_wordpairs(examples)\n",
    "    bigrams_ghub = generate_wordpairs_ghub(examples)\n",
    "    if not bigrams:\n",
    "        print(f'Skipping bad conversation: {conversation}')\n",
    "        continue\n",
    "    bigrams = remove_duplicates(bigrams)\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        seq_length, start_onset, end_onset, n_bins = calculate_windows_params(bigram, signal_param_dict)\n",
    "        \n",
    "        if seq_length <= 0:\n",
    "            continue\n",
    "            \n",
    "        train_seq_lengths.append(seq_length)\n",
    "        \n",
    "        if test_for_bad_window(start_onset, end_onset, ecogs.shape, window_fs):\n",
    "            continue\n",
    "\n",
    "        labels.append(add_begin_end_tokens(bigram[0], vocab, begin_tok, end_tok))  # put the sentence in the label vector/list\n",
    "        word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32) \n",
    "        for i, f in enumerate(np.array_split(ecogs[start_onset:end_onset,:], n_bins, axis=0)):\n",
    "            word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "\n",
    "        #TODO: Data Augmentation\n",
    "        signals.append(word_signal)\n",
    "        \n",
    "print('final')\n",
    "assert len(labels) == len(signals), \"Bad Shape for Lengths\"\n",
    "x_train = signals\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65037"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping no bigrams: /scratch/gpfs/hgazula/676-conversations/NY676_618_Part8_conversation1\n",
      "final\n"
     ]
    }
   ],
   "source": [
    "conversations = VALID_CONV\n",
    "convs = [(conv_dir + conv_name, '/misc/*datum_%s.txt' % ds, idx)\\\n",
    "         for idx, (conv_dir, convs, ds) in enumerate(zip(conv_dirs, conversations, datum_suffix))\\\n",
    "         for conv_name in convs]\n",
    "\n",
    "signals, labels = [], []\n",
    "for (conversation, suffix, idx) in convs:\n",
    "    # Check if files exists, if it doesn't go to next\n",
    "    datum_fn = glob.glob(conversation + suffix)[0]\n",
    "    if not datum_fn:\n",
    "        print('File DNE: ', conversation + suffix)\n",
    "        continue\n",
    "        \n",
    "    # Extract electrode data\n",
    "    ecogs = return_electrode_array(conversation, suffix)\n",
    "    if not ecogs.size:\n",
    "        print(f'Skipping bad conversation: {conversation}')\n",
    "        continue\n",
    "\n",
    "    examples = return_examples_spm(datum_fn, delimiter, vocab, exclude_words)\n",
    "    bigrams = generate_wordpairs(examples)\n",
    "    if not bigrams:\n",
    "        print(f'Skipping no bigrams: {conversation}')\n",
    "        continue\n",
    "    bigrams = remove_duplicates(bigrams)\n",
    "    \n",
    "    valid_seq_lengths = 0\n",
    "    for bigram in bigrams:\n",
    "        seq_length, start_onset, end_onset, n_bins = calculate_windows_params(bigram, signal_param_dict)\n",
    "        \n",
    "        if seq_length <=0:\n",
    "            continue\n",
    "            \n",
    "        if test_for_bad_window(start_onset, end_onset, ecogs.shape, window_fs):\n",
    "            continue\n",
    "\n",
    "        labels.append(add_begin_end_tokens(bigram[0], vocab, begin_tok, end_tok))  # put the sentence in the label vector/list\n",
    "        word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32) \n",
    "        for i, f in enumerate(np.array_split(ecogs[start_onset:end_onset,:], n_bins, axis=0)):\n",
    "            word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "\n",
    "        #TODO: Data Augmentation\n",
    "        signals.append(word_signal)\n",
    "        \n",
    "print('final')\n",
    "assert len(labels) == len(signals), \"Bad Shape for Lengths\"\n",
    "x_valid = signals\n",
    "y_valid = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting train and validation data to Loader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "### Pytorch Dataset wrapper\n",
    "class Brain2enDataset(Dataset):\n",
    "    \"\"\"Brainwave-to-English Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, signals, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            signals (list): brainwave examples.\n",
    "            labels (list): english examples.\n",
    "        \"\"\"\n",
    "        global oov_token, vocab\n",
    "\n",
    "        assert(len(signals) == len(labels))\n",
    "        indices = [(i, len(signals[i]), len(labels[i]))\\\n",
    "                   for i in range(len(signals))]\n",
    "        indices.sort(key=lambda x: (x[1], x[2], x[0]))\n",
    "        self.examples = []\n",
    "        self.max_seq_len = 0\n",
    "        self.max_sent_len = 0\n",
    "        self.train_freq = Counter()\n",
    "        c = 0\n",
    "        for i in indices:\n",
    "            if i[1] > 384 or i[2] < 4 or i[2] > 128:\n",
    "                c += 1\n",
    "                continue\n",
    "            lab = labels[i[0]]\n",
    "            self.train_freq.update(lab)\n",
    "            lab = torch.tensor(lab).long()\n",
    "            self.examples.append((torch.from_numpy(signals[i[0]]).float(), lab))\n",
    "            self.max_seq_len = max(self.max_seq_len, i[1])\n",
    "            self.max_sent_len = max(self.max_sent_len, len(lab))\n",
    "        print(\"Skipped\", c, \"examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "### Create a mask for subsequent positions and a mask for padding\n",
    "def masks(labels):\n",
    "    global pad_token, vocab\n",
    "    pos_mask = (torch.triu(torch.ones(labels.size(1), labels.size(1))) == 1).transpose(0, 1).unsqueeze(0)\n",
    "    pos_mask = pos_mask.float().masked_fill(pos_mask == 0,\n",
    "        float('-inf')).masked_fill(pos_mask == 1, float(0.0))\n",
    "    pad_mask = labels == vocab[pad_token]\n",
    "    return pos_mask, pad_mask\n",
    "\n",
    "\n",
    "### Batch padding method\n",
    "def pad_batch(batch):\n",
    "    global end_token, pad_token, vocab\n",
    "    src = pad_sequence([batch[i][0] for i in range(len(batch))], batch_first=True, padding_value=0.)\n",
    "    labels = pad_sequence([batch[i][1] for i in range(len(batch))], batch_first=True, padding_value=vocab[pad_token])\n",
    "    trg = torch.zeros(labels.size(0), labels.size(1), len(vocab)).scatter_(2, labels.unsqueeze(-1), 1)\n",
    "    trg, trg_y = trg[:,:-1,:], labels[:,1:]\n",
    "    pos_mask, pad_mask = masks(trg_y)\n",
    "    return src, trg, trg_y, pos_mask, pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch.utils.data as data\n",
    "\n",
    "### Shuffle labels if required\n",
    "if args.shuffle:\n",
    "    print(\"Shuffling labels\")\n",
    "    np.random.shuffle(y_train)\n",
    "    np.random.shuffle(y_valid)\n",
    "train_ds = Brain2enDataset(x_train, y_train)\n",
    "print(\"Number of training signals: \", len(train_ds))\n",
    "valid_ds = Brain2enDataset(x_valid, y_valid)\n",
    "print(\"Number of validation signals: \", len(valid_ds))\n",
    "train_dl = data.DataLoader(train_ds, batch_size=args.batch_size,\n",
    "                           shuffle=True, num_workers=CONFIG[\"num_cpus\"],\n",
    "                           collate_fn=pad_batch)\n",
    "valid_dl = data.DataLoader(valid_ds, batch_size=args.batch_size,\n",
    "                           num_workers=CONFIG[\"num_cpus\"],\n",
    "                           collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Default models and parameters\n",
    "DEFAULT_MODELS = {\n",
    "    \"ConvNet10\": (len(vocab),),\n",
    "    \"PITOM\": (len(vocab), len(args.electrodes)*len(args.subjects)),\n",
    "    \"MeNTALmini\": (len(args.electrodes)*len(args.subjects), len(vocab),\\\n",
    "                        args.tf_dmodel, args.tf_nhead, args.tf_nlayer,\\\n",
    "                        args.tf_dff, args.tf_dropout),\n",
    "    \"MeNTAL\": (len(args.electrodes)*len(args.subjects), len(vocab),\n",
    "                         args.tf_dmodel, args.tf_nhead, args.tf_nlayer,\\\n",
    "                         args.tf_dff, args.tf_dropout)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "\n",
    "if args.init_model is None:\n",
    "    if args.model in DEFAULT_MODELS:\n",
    "        print(\"Building default model: %s\" % args.model, end=\"\")\n",
    "        model_class = globals()[args.model]\n",
    "        model = model_class(*(DEFAULT_MODELS[args.model]))\n",
    "    else:\n",
    "        print(\"Building custom model: %s\" % args.model, end=\"\")\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    model_name = \"%s%s.pt\" % (SAVE_DIR, args.model)\n",
    "    if os.path.isfile(model_name):\n",
    "        model = torch.load(model_name)\n",
    "        model = model.module if hasattr(model, 'module') else model\n",
    "        print(\"Loaded initial model: %s \" % args.model)\n",
    "    else:\n",
    "        print(\"No models found in: \", SAVE_DIR)\n",
    "        sys.exit(1)\n",
    "print(\" with %d trainable parameters\"\n",
    "    % sum([p.numel() for p in model.parameters() if p.requires_grad]))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "\n",
    "# Initialize loss and optimizer\n",
    "# weights = torch.ones(n_classes)\n",
    "# max_freq = -1.\n",
    "# for i in range(n_classes):\n",
    "#     max_freq = max(max_freq, word2freq[vocab[i]])\n",
    "#     weights[i] = 1./float(word2freq[vocab[i]])\n",
    "# weights = weights*max_freq\n",
    "# print(sorted([(vocab[i], round(float(weights[i]),1)) for i in range(n_classes)],\n",
    "#              key=lambda x: x[1]))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "step_size = int(math.ceil(len(train_ds)/args.batch_size))\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.98),\n",
    "#                         eps=1e-9, weight_decay=args.weight_decay)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=args.lr,\n",
    "#                         weight_decay=args.weight_decay)\n",
    "optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "# optimizer = NoamOpt(args.tf_dmodel, 0.2, 5*step_size,\n",
    "#             optim.Adam(model.parameters(), lr=0., betas=(0.9, 0.98), eps=1e-9))\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "#                                            milestones=[10*step_size,20*step_size, 40*step_size],\n",
    "#                                            gamma=0.2)\n",
    "# scheduler = get_cosine_schedule_with_warmup(optimizer, 10*step_size,\n",
    "#                                             args.epochs*step_size, num_cycles=2.5)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model and loss to GPUs\n",
    "if args.gpus:\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "    if args.gpus > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "# Batch chunk size to send to single GPU\n",
    "# import math\n",
    "# chunk_size = int(math.ceil(float(args.batch_size)/max(1,args.gpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "################################################################################\n",
    "#\n",
    "# Brain2En > Training and Evaluation Utilities\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "### Libraries\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import auc, confusion_matrix, roc_curve\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# Optimization Classes and Methods\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "\n",
    "CLIP_NORM = 1.0\n",
    "REGEX = re.compile('[^a-zA-Z]')\n",
    "\n",
    "### NOAM Optimizer\n",
    "class NoamOpt:\n",
    "    \"Optimizer wrapper implementing learning scheme\"\n",
    "    def __init__(self, d_model, prefactor, warmup, optimizer):\n",
    "        self.d_model = d_model\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup = warmup\n",
    "        self.prefactor = prefactor\n",
    "        self._step = 0\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step = None):\n",
    "        \"Implement learning rate warmup scheme\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.prefactor * (self.d_model ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "### Regularization by Label Smoothing\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implements label smoothing on a multiclass target.\"\n",
    "    def __init__(self, criterion, size, pad_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.pad_idx = pad_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert(x.size(1) == self.size)\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1,\n",
    "            target.data.unsqueeze(1).long(),\n",
    "            self.confidence)\n",
    "        true_dist[:, self.pad_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.pad_idx)\n",
    "        if mask.sum() > 0 and len(mask) > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "### Single GPU Loss Computation\n",
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, criterion, opt=None, scheduler=None):\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def __call__(self, x, y, val=False):\n",
    "        loss = self.criterion(x.view(-1, x.size(-1)),\\\n",
    "                              y.view(-1))\n",
    "        if not val:\n",
    "            loss.backward()\n",
    "            if self.opt is not None:\n",
    "                self.opt.step()\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "        return loss.data.item()\n",
    "\n",
    "### Multi GPU Loss Computation\n",
    "class MultiGPULossCompute:\n",
    "    \"A multi-gpu loss compute and train function.\"\n",
    "    def __init__(self, criterion, devices, opt=None, scheduler=None, chunk_size=5):\n",
    "        # Send out to different gpus.\n",
    "        self.criterion = nn.parallel.replicate(criterion,\n",
    "                                               devices=devices)\n",
    "        self.opt = opt\n",
    "        self.scheduler = scheduler\n",
    "        self.devices = devices\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __call__(self, x, y, val=False):\n",
    "        total = 0.0\n",
    "        out_scatter = nn.parallel.scatter(out,\n",
    "                                          target_gpus=self.devices)\n",
    "        out_grad = [[] for _ in out_scatter]\n",
    "        targets = nn.parallel.scatter(targets,\n",
    "                                      target_gpus=self.devices)\n",
    "\n",
    "        # Divide generating into chunks.\n",
    "        chunk_size = self.chunk_size\n",
    "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
    "            # Predict distributions\n",
    "            out_column = [[Variable(o[:, i:i+chunk_size].data,\n",
    "                                    requires_grad=self.opt is not None)]\n",
    "                           for o in out_scatter]\n",
    "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
    "\n",
    "            # Compute loss.\n",
    "            y = [(g.contiguous().view(-1, g.size(-1)),\n",
    "                  t[:, i:i+chunk_size].contiguous().view(-1))\n",
    "                 for g, t in zip(gen, targets)]\n",
    "            loss = nn.parallel.parallel_apply(self.criterion, y)\n",
    "\n",
    "            # Sum and normalize loss\n",
    "            l = nn.parallel.gather(loss,\n",
    "                                   target_device=self.devices[0])\n",
    "            l = l.sum()[0]\n",
    "            total += l.data[0]\n",
    "\n",
    "            # Backprop loss to output of transformer\n",
    "            if not val and self.opt is not None:\n",
    "                l.backward()\n",
    "                for j, l in enumerate(loss):\n",
    "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
    "        if not val:\n",
    "            if self.opt is not None:\n",
    "                out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n",
    "                o1 = out\n",
    "                o2 = nn.parallel.gather(out_grad,\n",
    "                                        target_device=self.devices[0])\n",
    "                o1.backward(gradient=o2)\n",
    "                self.opt.step()\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "        return total\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# Training Methods\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "\n",
    "### Training loop\n",
    "def train(data_iter, model, criterion, devices, opt,\n",
    "          scheduler=None, seq2seq=False, pad_idx=-1):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    total_loss = 0.\n",
    "    total_acc = 0.\n",
    "    count, batch_count = 0, 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # Prevent gradient accumulation\n",
    "        model.zero_grad()\n",
    "        src = batch[0].cuda()\n",
    "        trg = batch[1].long().to(src.device)\n",
    "        if seq2seq:\n",
    "            # trg_y = batch[2].long().to(src.device)\n",
    "            # trg_pos_mask, trg_pad_mask = batch[3].to(src.device), batch[4].to(src.device)\n",
    "            # out, trg_y = model.forward(src, trg, trg_y, trg_pos_mask, trg_pad_mask)\n",
    "            # # Fix asymmetrical load on single GPU by computing loss in parallel\n",
    "            # out_scatter = nn.parallel.scatter(out, target_gpus=devices)\n",
    "            # out_grad = [[] for _ in out_scatter]\n",
    "            # trg_y_scatter = nn.parallel.scatter(trg_y, target_gpus=devices)\n",
    "            # chunk_size = int(math.ceil(out_scatter[0].size(0)/len(devices)))\n",
    "            # for i in range(0, out_scatter[0].size(0), chunk_size):\n",
    "            #     out_scatter_chunk = [Variable(o[i:end], requires_grad=True)\\\n",
    "            #                          for o in out_scatter if (end := min(i+chunk_size,len(o))) > i]\n",
    "            #     trg_y_scatter_chunk = [t[i:end] for t in trg_y_scatter\\\n",
    "            #                            if (end := min(i+chunk_size, len(t))) > i]\n",
    "            #     idx_scatter = [(t != pad_idx).nonzero(as_tuple=True) for t in trg_y_scatter_chunk]\n",
    "            #     y = [(o.contiguous().view(-1, o.size(-1)), t.contiguous().view(-1)) for o, t in zip(out_scatter_chunk, trg_y_scatter_chunk)]\n",
    "            #     print(len(criterion), len(y))\n",
    "            #     print([(o.size(), t.size()) for o, t in y])\n",
    "            #     loss = nn.parallel.parallel_apply(criterion[:len(y)], y)\n",
    "            #     loss = [l.unsqueeze(-1) for l in loss]\n",
    "            #     num_dev = len(loss)\n",
    "            #     # print(num_dev)\n",
    "            #     loss = nn.parallel.gather(loss, target_device=devices[0])\n",
    "            #     loss = loss.sum()\n",
    "            #     total_loss += float(loss.item() / num_dev)\n",
    "            #     loss.backward()\n",
    "            #     for j in range(num_dev):\n",
    "            #         out_grad[j].append(out_scatter_chunk[j].grad.data.clone())\n",
    "            #     out_scatter_chunk = [torch.argmax(o[idx], dim=1) for idx, o in zip(idx_scatter, out_scatter_chunk)]\n",
    "            #     trg_y_scatter_chunk = [t[idx] for idx, t in zip(idx_scatter, trg_y_scatter_chunk)]\n",
    "            #     total_acc += sum([float((o == t).sum()) for o, t in zip(out_scatter_chunk, trg_y_scatter_chunk)])\n",
    "            #     count += sum(int(o.size(0)) for o in out_scatter_chunk)\n",
    "            #     # del out_scatter_chunk, trg_y_scatter_chunk, idx_scatter, y, loss\n",
    "            #     print(total_loss, total_acc / count)\n",
    "            #     sys.stdout.flush()\n",
    "            # print(\"Here now\")\n",
    "            # out_grad = [Variable(torch.cat(og, dim=0)) for og in out_grad]\n",
    "            # print(out.size(), trg_y.size())\n",
    "            # print([o.size() for o in out_grad])\n",
    "            # sys.stdout.flush()\n",
    "            # out.backward(gradient=nn.parallel.gather(out_grad,\n",
    "            #                                          target_device=devices[0]))\n",
    "            # del src, trg, trg_y, trg_pos_mask, trg_pad_mask, out, out_grad\n",
    "            # print(\"brah\")\n",
    "            # loss = criterion(out.view(-1, out.size(-1)), trg_y.view(-1))\n",
    "            # loss.backward()\n",
    "            trg_y = batch[2].long().to(src.device)\n",
    "            trg_pos_mask, trg_pad_mask = batch[3].to(src.device), batch[4].to(src.device)\n",
    "            # Perform loss computation during forward pass for parallelism\n",
    "            out, trg_y, loss = model.forward(src, trg, trg_pos_mask, trg_pad_mask, trg_y, criterion)\n",
    "            idx = (trg_y != pad_idx).nonzero(as_tuple=True)\n",
    "            total_loss += loss.data.item()\n",
    "            out = out[idx]\n",
    "            trg_y = trg_y[idx]\n",
    "            out = torch.argmax(out, dim=1)\n",
    "            total_acc += float((out == trg_y).sum())\n",
    "            opt.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            # total_loss += loss.data.item()\n",
    "            # out = out[idx]\n",
    "            # trg_y = trg_y[idx]\n",
    "            # out = torch.argmax(out, dim=1)\n",
    "            # total_acc += float((out == trg_y).sum())\n",
    "            # print(\"hereo\")\n",
    "            # sys.stdout.flush()\n",
    "        else:\n",
    "            out = model.forward(src)\n",
    "            loss = criterion(out.view(-1, out.size(-1)), trg.view(-1))\n",
    "            loss.backward()\n",
    "            if opt is not None:\n",
    "                opt.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            total_loss += loss.data.item()\n",
    "            total_acc += float((torch.argmax(out, dim=1) == trg).sum())\n",
    "            # count += int(out.size(0))\n",
    "        # Prevent gradient blowup\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "        count += int(out.size(0))\n",
    "        batch_count += 1\n",
    "    total_loss /= batch_count\n",
    "    total_acc /= count\n",
    "    elapsed = (time.time() - start_time) * 1000. / batch_count\n",
    "    perplexity = float('inf')\n",
    "    try:\n",
    "        perplexity = math.exp(total_loss)\n",
    "    except:\n",
    "        pass\n",
    "    print('loss {:5.3f} | accuracy {:5.3f} | perplexity {:3.2f} | ms/batch {:5.2f}'.format(total_loss, total_acc, perplexity, elapsed), end='')\n",
    "    return total_loss, total_acc\n",
    "\n",
    "### Validation loop\n",
    "def valid(data_iter, model, criterion,\n",
    "          temperature=1.0, n_samples=10, seq2seq=False, pad_idx=-1):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_acc = 0.\n",
    "    total_sample_rank_acc = 0.\n",
    "    batch_count, count = 0, 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        src = batch[0].cuda()\n",
    "        trg = batch[1].long().to(src.device)\n",
    "        if seq2seq:\n",
    "            trg_y = batch[2].long().to(src.device)\n",
    "            trg_pos_mask, trg_pad_mask = batch[3].to(src.device), batch[4].to(src.device)\n",
    "            out, trg_y, loss = model.forward(src, trg, trg_pos_mask, trg_pad_mask, trg_y, criterion)\n",
    "            idx = (trg_y != pad_idx).nonzero(as_tuple=True)\n",
    "            total_loss += loss.data.item()\n",
    "            out = out[idx]\n",
    "            trg_y = trg_y[idx]\n",
    "            out_top1 = torch.argmax(out, dim=1)\n",
    "            total_acc += float((out_top1 == trg_y).sum())\n",
    "            out = F.softmax(out/temperature, dim=1)\n",
    "            samples = torch.multinomial(out, n_samples)\n",
    "            pred = torch.zeros(samples.size(0)).cuda()\n",
    "            for j in range(len(pred)):\n",
    "                pred[j] = samples[j,torch.argmax(out[j,samples[j]])]\n",
    "            total_sample_rank_acc += float((pred == trg_y).sum())\n",
    "        else:\n",
    "            out = model.forward(src)\n",
    "            loss = criterion(out.view(-1, out.size(-1)), trg.view(-1))\n",
    "            total_loss += loss.data.item()\n",
    "            total_acc += float((torch.argmax(out, dim=1) == trg).sum())\n",
    "            out = F.softmax(out/temperature, dim=1)\n",
    "            samples = torch.multinomial(out, n_samples)\n",
    "            pred = torch.zeros(samples.size(0)).cuda()\n",
    "            for j in range(len(pred)):\n",
    "                pred[j] = samples[j,torch.argmax(out[j,samples[j]])]\n",
    "            total_sample_rank_acc += float((pred == trg).sum())\n",
    "        count += int(out.size(0))\n",
    "        batch_count += 1\n",
    "    total_loss /= batch_count\n",
    "    total_acc /= count\n",
    "    total_sample_rank_acc /= count\n",
    "    perplexity = float('inf')\n",
    "    try:\n",
    "        perplexity = math.exp(total_loss)\n",
    "    except:\n",
    "        pass\n",
    "    print('loss {:5.3f} | accuracy {:5.3f} | sample-rank acc {:5.3f} | perplexity {:3.2f}'.format(total_loss, total_acc, total_sample_rank_acc, perplexity))\n",
    "    return total_loss, total_acc\n",
    "\n",
    "### Plot train/val loss and accuracy and save figures\n",
    "def plot_training(history, save_dir, title='', val=True):\n",
    "    plt.plot(history[\"train_loss\"])\n",
    "    if val:\n",
    "        plt.plot(history[\"valid_loss\"])\n",
    "    plt.title('Model loss: %s' % title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(save_dir + 'loss.png')\n",
    "    plt.clf()\n",
    "    plt.plot(history[\"train_acc\"])\n",
    "    if val:\n",
    "        plt.plot(history[\"valid_acc\"])\n",
    "    plt.title('Model accuracy: %s' % title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(save_dir + 'accuracy.png')\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# Evaluation Methods\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "\n",
    "### Choose point of minimum distance to an ideal point,\n",
    "### (For ROC: (0,1); for PR: (1,1)).\n",
    "def best_threshold(X, Y, T, best_x=0., best_y=1.):\n",
    "    min_d, min_i = np.inf, 0\n",
    "    for i, (x, y) in enumerate(zip(X, Y)):\n",
    "        d = np.sqrt((best_x-x)**2 + (best_y-y)**2)\n",
    "        if d < min_d:\n",
    "            min_d, min_i = d, i\n",
    "    return X[min_i], Y[min_i], T[min_i]\n",
    "\n",
    "### Evaluate ROC performance of the model\n",
    "### (predictions, labels of shape (n_examples, n_classes))\n",
    "def evaluate_roc(predictions, labels, i2w, train_freqs, save_dir, do_plot,\n",
    "                 given_thresholds=None, title='', suffix='', min_train=10,\n",
    "                 tokens_to_remove=[]):\n",
    "    assert(predictions.shape == labels.shape)\n",
    "    lines, scores, word_freqs = [], [], []\n",
    "    n_examples, n_classes = predictions.shape\n",
    "    thresholds = np.full(n_classes, np.nan)\n",
    "    rocs, fprs, tprs = {}, [], []\n",
    "\n",
    "    # Create directory for plots if required\n",
    "    if do_plot:\n",
    "        roc_dir = save_dir + 'rocs/'\n",
    "        if not os.path.isdir(roc_dir): os.mkdir(roc_dir)\n",
    "\n",
    "    # Go over each class and compute AUC\n",
    "    for i in range(n_classes):\n",
    "        if i2w[i] in tokens_to_remove:\n",
    "            continue\n",
    "        train_count = train_freqs[i]\n",
    "        n_true = np.count_nonzero(labels[:,i])\n",
    "        if train_count < 1 or n_true == 0: continue\n",
    "        word = i2w[i]\n",
    "        probs = predictions[:,i]\n",
    "        c_labels = labels[:,i]\n",
    "        fpr, tpr, thresh = roc_curve(c_labels, probs)\n",
    "        if given_thresholds is None:\n",
    "            x, y, threshold = best_threshold(fpr, tpr, thresh)\n",
    "        else:\n",
    "            x, y, threshold = 0, 0, given_thresholds[i]\n",
    "        thresholds[i] = threshold\n",
    "        score = auc(fpr, tpr)\n",
    "        scores.append(score)\n",
    "        word_freqs.append(train_count)\n",
    "        rocs[word] = score\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "        y_pred = probs >= threshold\n",
    "        tn, fp, fn, tp = confusion_matrix(c_labels, y_pred).ravel()\n",
    "        lines.append('%s\\t%3d\\t%3d\\t%.3f\\t%d\\t%d\\t%d\\t%d\\n' \\\n",
    "                % (word, n_true, train_count, score, tp, fp, fn, tn))\n",
    "        if do_plot:\n",
    "            fig, axes = plt.subplots(1,2, figsize=(16,6))\n",
    "            axes[0].plot(fpr, tpr, color='darkorange', lw=2, marker='.')\n",
    "            axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            axes[0].plot(x, y, marker='o', color='blue')\n",
    "            axes[0].set_xlim([0.0, 1.0])\n",
    "            axes[0].set_ylim([0.0, 1.05])\n",
    "            axes[0].set_xlabel('False Positive Rate')\n",
    "            axes[0].set_ylabel('True Positive Rate')\n",
    "            h1 = probs[c_labels == 1].reshape(-1)\n",
    "            h2 = probs[c_labels == 0].reshape(-1)\n",
    "            axes[1].hist(h2, bins=20, color='orange',\n",
    "                             alpha=0.5, label='Neg. Examples')\n",
    "            #axes[1].twinx().hist(h1, bins=50, alpha=0.5, label='Pos. Examples')\n",
    "            axes[1].hist(h1, bins=50, alpha=0.5, label='Pos. Examples')\n",
    "            axes[1].axvline(threshold, color='k')\n",
    "            axes[1].set_xlabel('Activation')\n",
    "            axes[1].set_ylabel('Frequency')\n",
    "            axes[1].legend()\n",
    "            axes[1].set_title('%d TP | %d FP | %d FN | %d TN'\\\n",
    "                              % (tp, fp, fn, tn))\n",
    "            fig.suptitle('ROC Curve | %s | AUC = %.3f | N = %d'\\\n",
    "                         % (word, score, n_true))\n",
    "            plt.savefig(roc_dir + '%s.png' % word)\n",
    "            fig.clear()\n",
    "            plt.close(fig)\n",
    "\n",
    "    # Compute statistics\n",
    "    scores, word_freqs = np.array(scores), np.array(word_freqs)\n",
    "    normed_freqs = word_freqs / word_freqs.sum()\n",
    "    avg_auc = scores.mean()\n",
    "    weighted_avg = (scores * normed_freqs).sum()\n",
    "    print('Avg AUC: %d\\t%.6f' % (scores.size, avg_auc))\n",
    "    print('Weighted Avg AUC: %d\\t%.6f' % (scores.size, weighted_avg))\n",
    "\n",
    "    # Write to file\n",
    "    with open(save_dir + 'aucs%s.txt' % suffix, 'w') as fout:\n",
    "        for line in lines:\n",
    "            fout.write(line)\n",
    "\n",
    "    # Plot histogram and AUC as a function of num of examples\n",
    "    _, ax = plt.subplots(1,1)\n",
    "    ax.scatter(word_freqs, scores, marker='.')\n",
    "    ax.set_xlabel('# examples')\n",
    "    ax.set_ylabel('AUC')\n",
    "    ax.set_title('%s | avg: %.3f | N = %d' % (title, weighted_avg, scores.size))\n",
    "    ax.set_yticks(np.arange(0., 1.1, 0.1))\n",
    "    ax.grid()\n",
    "    plt.savefig(save_dir + 'roc-auc-examples.png', bbox_inches='tight')\n",
    "\n",
    "    _, ax = plt.subplots(1,1)\n",
    "    ax.hist(scores, bins=20)\n",
    "    ax.set_xlabel('AUC')\n",
    "    ax.set_ylabel('# labels')\n",
    "    ax.set_title('%s | avg: %.3f | N = %d' % (title, weighted_avg, scores.size))\n",
    "    ax.set_xticks(np.arange(0., 1., 0.1))\n",
    "    plt.savefig(save_dir + 'roc-auc.png', bbox_inches='tight')\n",
    "\n",
    "    _, ax = plt.subplots(1,1)\n",
    "    for fpr, tpr in zip(fprs, tprs):\n",
    "        ax.plot(fpr, tpr, lw=1)\n",
    "    ax.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('%s | avg: %.3f | N = %d' % (title, weighted_avg, scores.size))\n",
    "    plt.savefig(save_dir + 'roc-auc-all.png', bbox_inches='tight')\n",
    "\n",
    "    return {\n",
    "        'rocauc_avg': avg_auc,\n",
    "        'rocauc_stddev': scores.std(),\n",
    "        'rocauc_w_avg': weighted_avg,\n",
    "        'rocauc_n': scores.size,\n",
    "        'rocs': rocs\n",
    "    }\n",
    "\n",
    "### Evaluate top-k performance of the model. (assumes activations can be\n",
    "### interpreted as probabilities).\n",
    "### (predictions, labels of shape (n_examples, n_classes))\n",
    "def evaluate_topk(predictions, labels, i2w, train_freqs, save_dir,\n",
    "                  min_train=10, prefix='', suffix='', tokens_to_remove=[]):\n",
    "    ranks = []\n",
    "    n_examples, n_classes = predictions.shape\n",
    "    fid = open(save_dir + 'guesses%s.csv' % suffix, 'w')\n",
    "    top1_uw, top5_uw, top10_uw = set(), set(), set()\n",
    "    accs, sizes = {}, {}\n",
    "    total_freqs = float(sum(train_freqs.values()))\n",
    "\n",
    "    # Go through each example and calculate its rank and top-k\n",
    "    for i in range(n_examples):\n",
    "        y_true_idx = labels[i]\n",
    "\n",
    "        if train_freqs[y_true_idx] < 1:\n",
    "            continue\n",
    "\n",
    "        word = i2w[y_true_idx]\n",
    "        if word in tokens_to_remove:\n",
    "            continue\n",
    "\n",
    "        # Get example predictions\n",
    "        ex_preds = np.argsort(predictions[i])[::-1]\n",
    "        rank = np.where(y_true_idx == ex_preds)[0][0]\n",
    "        ranks.append(rank)\n",
    "\n",
    "        fid.write('%s,%d,' % (word, rank))\n",
    "        fid.write(','.join(i2w[j] for j in ex_preds[:10]))\n",
    "        fid.write('\\n')\n",
    "\n",
    "        if rank == 0:\n",
    "            top1_uw.add(ex_preds[0])\n",
    "        elif rank < 5:\n",
    "            top5_uw.update(ex_preds[:5])\n",
    "        elif rank < 10:\n",
    "            top10_uw.update(ex_preds[:10])\n",
    "\n",
    "        if word not in accs:\n",
    "            accs[word] = float(rank == 0)\n",
    "            sizes[y_true_idx] = 1.\n",
    "        else:\n",
    "            accs[word] += float(rank == 0)\n",
    "            sizes[y_true_idx] += 1.\n",
    "    for idx in sizes:\n",
    "        word = i2w[idx]\n",
    "        chance_acc = float(train_freqs[idx]) / total_freqs * 100.\n",
    "        if sizes[idx] > 0:\n",
    "            rounded_acc = round(accs[word] / sizes[idx] * 100, 3)\n",
    "            accs[word] = (rounded_acc, chance_acc, rounded_acc - chance_acc)\n",
    "        else:\n",
    "            accs[word] = (0., chance_acc, -chance_acc)\n",
    "    accs = sorted(accs.items(), key=lambda x: -x[1][2])\n",
    "\n",
    "    fid.close()\n",
    "    print('Top1 #Unique:', len(top1_uw))\n",
    "    print('Top5 #Unique:', len(top5_uw))\n",
    "    print('Top10 #Unique:', len(top10_uw))\n",
    "\n",
    "    n_examples = len(ranks)\n",
    "    ranks = np.array(ranks)\n",
    "    top1 = sum(ranks == 0) / (1e-12 + len(ranks)) * 100\n",
    "    top5 = sum(ranks < 5) / (1e-12 + len(ranks)) * 100\n",
    "    top10 = sum(ranks < 10) / (1e-12 + len(ranks)) * 100\n",
    "\n",
    "    # Calculate chance levels based on training word frequencies\n",
    "    freqs = Counter(labels)\n",
    "    freqs = np.array([freqs[i] for i,_ in train_freqs.most_common()])\n",
    "    freqs = freqs[freqs > 0]\n",
    "    chances = (freqs / freqs.sum()).cumsum() * 100\n",
    "\n",
    "    # Print and write to file\n",
    "    if suffix is not None:\n",
    "        with open(save_dir + 'topk%s.txt' % suffix, 'w') as fout:\n",
    "            line = 'n_classes: %d\\nn_examples: %d' % (n_classes, n_examples)\n",
    "            print(line)\n",
    "            fout.write(line + '\\n')\n",
    "            line = 'Top-1\\t%.4f %% (%.2f %%)' % (top1, chances[0])\n",
    "            print(line)\n",
    "            fout.write(line + '\\n')\n",
    "            line = 'Top-5\\t%.4f %% (%.2f %%)' % (top5, chances[4])\n",
    "            print(line)\n",
    "            fout.write(line + '\\n')\n",
    "            line = 'Top-10\\t%.4f %% (%.2f %%)' % (top10, chances[9])\n",
    "            print(line)\n",
    "            fout.write(line + '\\n')\n",
    "\n",
    "    return {\n",
    "        prefix + 'top1': top1,\n",
    "        prefix + 'top5': top5,\n",
    "        prefix + 'top10': top10,\n",
    "        prefix + 'top1_chance':  chances[0],\n",
    "        prefix + 'top5_chance':  chances[4],\n",
    "        prefix + 'top10_chance': chances[9],\n",
    "        prefix + 'top1_above':  (top1 - chances[0]) / chances[0],\n",
    "        prefix + 'top5_above':  (top5 - chances[4]) / chances[4],\n",
    "        prefix + 'top10_above': (top10 - chances[9]) / chances[9],\n",
    "        prefix + 'top1_n_uniq_correct': len(top1_uw),\n",
    "        prefix + 'top5_n_uniq_correct': len(top5_uw),\n",
    "        prefix + 'top10_n_uniq_correct': len(top10_uw),\n",
    "        prefix + 'word_accuracies': accs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Training and evaluation script\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "print(\"Training on %d GPU(s) with batch_size %d for %d epochs\"\\\n",
    "    % (args.gpus, args.batch_size, args.epochs))\n",
    "print(\"=\" * CONFIG[\"print_pad\"])\n",
    "sys.stdout.flush()\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model = model\n",
    "history = {'train_loss': [],\n",
    "           'train_acc': [],\n",
    "           'valid_loss': [],\n",
    "           'valid_acc': []}\n",
    "\n",
    "# train_loss_compute = SimpleLossCompute(criterion,\n",
    "#                                        opt=optimizer, scheduler=scheduler)\n",
    "# valid_loss_compute = SimpleLossCompute(criterion, opt=None, scheduler=None)\n",
    "\n",
    "epoch = 0\n",
    "model_name = \"%s%s.pt\" % (SAVE_DIR, args.model)\n",
    "\n",
    "# totalfreq = float(sum(train_ds.train_freq.values()))\n",
    "# print(sorted(((i2w[l],f/totalfreq) for l, f in train_ds.train_freq.most_common()), key=lambda x: -x[1]))\n",
    "\n",
    "# Run training and validation for args.epochs epochs\n",
    "lr = args.lr\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    print('| train | epoch %d | ' % epoch, end='')\n",
    "    train_loss, train_acc = train(train_dl, model,\n",
    "                                  criterion, list(range(args.gpus)),\n",
    "                                  optimizer, scheduler=scheduler,\n",
    "                                  seq2seq=not classify,\n",
    "                                  pad_idx=vocab[CONFIG[\"pad_token\"]] if not classify else -1)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'lr' in param_group:\n",
    "            print(' | lr {:1.2E}'.format(param_group['lr']))\n",
    "            break\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    print('| valid | epoch %d | ' % epoch, end='')\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = valid(valid_dl, model,\n",
    "                                      criterion, temperature=args.temp,\n",
    "                                      seq2seq=not classify,\n",
    "                                      pad_idx=vocab[CONFIG[\"pad_token\"]] if not classify else -1)\n",
    "    history['valid_loss'].append(valid_loss)\n",
    "    history['valid_acc'].append(valid_acc)\n",
    "    print('|' + '-'*(CONFIG[\"print_pad\"]-2) + '|')\n",
    "    # Store best model so far\n",
    "    if valid_loss < best_val_loss:\n",
    "        best_model, best_val_loss = model, valid_loss\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model_to_save = best_model.module\\\n",
    "                if hasattr(best_model, 'module') else best_model\n",
    "            torch.save(model_to_save, model_name)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # if epoch > 10 and valid_loss > max(history['valid_loss'][-3:]):\n",
    "    #     lr /= 2.\n",
    "    #     for param_group in optimizer.param_groups:\n",
    "    #         param_group['lr'] = lr\n",
    "\n",
    "# Plot loss,accuracy vs. time and save figures\n",
    "plot_training(history, SAVE_DIR, title=\"%s_lr%s\" % (args.model, args.lr))\n",
    "\n",
    "# Save best model found\n",
    "# print(\"Saving best model as %s.pt\" % args.model)\n",
    "# sys.stdout.flush()\n",
    "\n",
    "if not args.no_eval and classify:\n",
    "\n",
    "    print(\"Evaluating predictions on test set\")\n",
    "    # Load best model\n",
    "    model = torch.load(model_name)\n",
    "    if args.gpus:\n",
    "        if args.gpus > 1: model = nn.DataParallel(model)\n",
    "        model.to(DEVICE)\n",
    "\n",
    "    start, end = 0, 0\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    all_preds = np.zeros((x_valid.size(0), n_classes), dtype=np.float32)\n",
    "    print('Allocating', np.prod(all_preds.shape)*5/1e9,'GB')\n",
    "\n",
    "    # Calculate all predictions on test set\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dl:\n",
    "            src, trg = batch[0].to(DEVICE), batch[1].to(DEVICE, dtype=torch.long)\n",
    "            end = start + src.size(0)\n",
    "            out = softmax(model(src))\n",
    "            all_preds[start:end, :] = out.cpu()\n",
    "            start = end\n",
    "\n",
    "    print(\"Calculated predictions\")\n",
    "\n",
    "    # Make categorical\n",
    "    n_examples = y_valid.shape[0]\n",
    "    categorical = np.zeros((n_examples, n_classes), dtype=np.float32)\n",
    "    categorical[np.arange(n_examples), y_valid] = 1\n",
    "\n",
    "    train_freq = Counter(y_train.tolist())\n",
    "\n",
    "    # Evaluate top-k\n",
    "    print(\"Evaluating top-k\")\n",
    "    sys.stdout.flush()\n",
    "    res = evaluate_topk(all_preds, y_valid.numpy(), i2w, train_freq,\n",
    "                        SAVE_DIR, suffix='-val',\n",
    "                        min_train=args.vocab_min_freq)\n",
    "\n",
    "    # Evaluate ROC-AUC\n",
    "    print(\"Evaluating ROC-AUC\")\n",
    "    sys.stdout.flush()\n",
    "    res.update(evaluate_roc(all_preds, categorical, i2w, train_freq,\n",
    "                            SAVE_DIR, do_plot=not args.no_plot,\n",
    "                            min_train=args.vocab_min_freq))\n",
    "    pprint(res.items())\n",
    "    print(\"Saving results\")\n",
    "    with open(SAVE_DIR + \"results.json\", \"w\") as fp:\n",
    "        json.dump(res, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.no_eval and not classify:\n",
    "\n",
    "    print(\"Evaluating predictions on test set\")\n",
    "    # Load best model\n",
    "    model = torch.load(model_name)\n",
    "    if args.gpus:\n",
    "        model.cuda()\n",
    "\n",
    "    all_preds, categorical, all_labs = [], [], []\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    # Calculate all predictions on test set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in valid_dl:\n",
    "            src, trg_y = batch[0].cuda(), batch[2].long().cuda()\n",
    "            trg_pos_mask, trg_pad_mask = batch[3].cuda().squeeze(), batch[4].cuda()\n",
    "            memory = model.encode(src)\n",
    "            y = torch.zeros(src.size(0), 1, len(vocab)).long().cuda()\n",
    "            y_sr = torch.zeros(src.size(0), 1, len(vocab)).long().cuda()\n",
    "            probs = torch.zeros(src.size(0), 1, len(vocab)).long().cuda()\n",
    "            y[:,:,vocab[CONFIG[\"begin_token\"]]] = 1\n",
    "            y_sr[:,:,vocab[CONFIG[\"begin_token\"]]] = 1\n",
    "            for i in range(trg_y.size(1)):\n",
    "                out = model.decode(memory, y,\n",
    "                                   trg_pos_mask[:y.size(1),:y.size(1)],\n",
    "                                   trg_pad_mask[:,:y.size(1)])[:,-1,:]\n",
    "                out = softmax(out/args.temp)\n",
    "                temp = torch.zeros(src.size(0), len(vocab)).long().cuda()\n",
    "                temp = temp.scatter_(1, torch.argmax(out, dim=1).unsqueeze(-1), 1)\n",
    "                y = torch.cat([y, temp.unsqueeze(1)], dim=1)\n",
    "                # probs = torch.cat([probs, out.unsqueeze(1)], dim=1)\n",
    "                samples = torch.multinomial(out, 20)\n",
    "                pred = torch.zeros(out.size(0)).long().cuda()\n",
    "                for j in range(len(samples)):\n",
    "                    pred[j] = samples[j,torch.argmax(out[j,samples[j]])]\n",
    "                temp = torch.zeros(pred.size(0), len(vocab)).long().cuda()\n",
    "                pred = temp.scatter_(1, pred.unsqueeze(-1), 1).unsqueeze(1)\n",
    "                y_sr = torch.cat([y_sr, pred], dim=1)\n",
    "            y, y_sr = y[:,1:,:], y_sr[:,1:,:]\n",
    "            idx = (trg_y != vocab[CONFIG[\"pad_token\"]]).nonzero(as_tuple=True)\n",
    "            lab = trg_y[idx]\n",
    "            cat = torch.zeros((lab.size(0), len(vocab)), dtype=torch.long).to(lab.device)\n",
    "            cat = cat.scatter_(1, lab.unsqueeze(-1), 1)\n",
    "            all_preds.extend(y[idx].cpu().numpy())\n",
    "            categorical.extend(cat.cpu().numpy())\n",
    "            all_labs.extend(lab.cpu().numpy())\n",
    "            print(\"Output: \", vocab.DecodeIds(torch.argmax(y[0], dim=1).tolist()))\n",
    "            print(\"Output_sr: \", vocab.DecodeIds(torch.argmax(y_sr[0], dim=1).tolist()))\n",
    "            print(\"Target: \", vocab.DecodeIds(trg_y[0].tolist()))\n",
    "            print()\n",
    "            print(\"Output: \", vocab.DecodeIds(torch.argmax(y[-1], dim=1).tolist()))\n",
    "            print(\"Output_sr: \", vocab.DecodeIds(torch.argmax(y_sr[-1], dim=1).tolist()))\n",
    "            print(\"Target: \", vocab.DecodeIds(trg_y[-1].tolist()))\n",
    "            # print(\"BLEU: \", sentence_bleu([target_sent], predicted_sent))\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    categorical = np.array(categorical)\n",
    "    all_labs = np.array(all_labs)\n",
    "    print(\"Calculated predictions\")\n",
    "\n",
    "    train_freq = train_ds.train_freq\n",
    "    i2w = {i:vocab.IdToPiece(i) for i in range(len(vocab))}\n",
    "    markers = [CONFIG[\"begin_token\"], CONFIG[\"end_token\"],\\\n",
    "               CONFIG[\"oov_token\"], CONFIG[\"pad_token\"]]\n",
    "\n",
    "    # Evaluate top-k\n",
    "    print(\"Evaluating top-k\")\n",
    "    sys.stdout.flush()\n",
    "    res = evaluate_topk(all_preds, all_labs, i2w, train_freq,\n",
    "                        SAVE_DIR, suffix='-val',\n",
    "                        min_train=args.vocab_min_freq,\n",
    "                        tokens_to_remove=markers)\n",
    "\n",
    "    # Evaluate ROC-AUC\n",
    "    print(\"Evaluating ROC-AUC\")\n",
    "    sys.stdout.flush()\n",
    "    res.update(evaluate_roc(all_preds, categorical, i2w, train_freq,\n",
    "                            SAVE_DIR, do_plot=not args.no_plot,\n",
    "                            min_train=args.vocab_min_freq,\n",
    "                            tokens_to_remove=markers))\n",
    "    pprint(res.items())\n",
    "    print(\"Saving results\")\n",
    "    with open(SAVE_DIR + \"results.json\", \"w\") as fp:\n",
    "        json.dump(res, fp, indent=4)\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for conversation, suffix, idx in convs[:1]:\n",
    "    \n",
    "#     # Check if files exists, if it doesn't go to next\n",
    "#     datum_fn = glob.glob(conversation + suffix)[0]\n",
    "#     if not datum_fn:\n",
    "#         print('File DNE: ', conversation + suffix)\n",
    "#         continue\n",
    "        \n",
    "#     # Extract electrode data\n",
    "# #     ecogs = return_electrode_array(conversation, suffix)\n",
    "#     ecogs = np.load('ecogs.npy')\n",
    "#     if not ecogs.size:\n",
    "#         print(f'Skipping bad conversation: {conversation}')\n",
    "#         continue\n",
    "\n",
    "#     # Read conversations and form examples\n",
    "#     old_size = len(signals)\n",
    "#     max_len = 0\n",
    "    \n",
    "#     examples = return_examples(datum_fn, delimiter, vocab)\n",
    "#     my_grams = []\n",
    "#     for first, second in zip(shorty, shorty[1:]):\n",
    "#         len1, len2 = len(first[0]), len(second[0])\n",
    "#         if first[1] and len1 == 2:  # if the first set already has two words and is speaker 1\n",
    "#             my_grams.append(first)\n",
    "#         if second[1] and len2 == 2:  # if the second set already has two words and is speaker 1\n",
    "#             my_grams.append(second)\n",
    "#         if first[1] and second[1]:\n",
    "#             if len1 == 1 and len2 == 1:\n",
    "#                 ak = (first[0] + second[0], True, first[2], second[3])\n",
    "#                 my_grams.append(ak)\n",
    "\n",
    "#     print(my_grams)\n",
    "#     df = pd.DataFrame(my_grams)\n",
    "#     df[['fw', 'sw']] = pd.DataFrame(df[0].tolist(), index= df.index) \n",
    "#     df.drop(columns=[0], inplace=True) \n",
    "#     df.drop_duplicates(inplace=True)\n",
    "#     df[0] = df[['fw', 'sw']].values.tolist()\n",
    "#     df.drop(columns=['fw', 'sw'], inplace=True)\n",
    "#     df = df[[0, 1, 2, 3]]\n",
    "#     my_exams = list(df.to_records(index=False))\n",
    "#     print(my_exams)\n",
    "    \n",
    "#     signals, labels = [], []\n",
    "#     speaker = False\n",
    "#     cur_sentence, start_onset, end_onset = [vocab[begin_token]], 0, 0\n",
    "#     ###################################################################\n",
    "# #     for example in examples:  # loop over each example\n",
    "# #         if example[1]:  # check if it is speaker 1\n",
    "# #             '''If this is the beginning of the sentence'''\n",
    "# #             if len(cur_sentence) == 1:  # if sentence length is 1 (begin token)\n",
    "# #                 onset, offset = float(x[2]), float(x[3])  # grab onset and offset of current word\n",
    "# #                 start_onset = onset + start_window  # calculate the window_begin (its before the onset) \n",
    "# #                 end_onset = offset + end_window  # calculate the window_end (its after the offset)\n",
    "# #                 if start_onset < 0 or start_onset > ecogs.shape[0]:  # check if onset is within limits\n",
    "# #                     continue  # move on to the next example\n",
    "# #                 if (end_onset < 0 or  # if the window_end is less than 0 or\n",
    "# #                     end_onset > ecogs.shape[0]  # if the window_end is outside the signal \n",
    "# #                     or end_onset - start_onset < window_fs):  # if there are not enough frames in the window\n",
    "# #                     continue  # move on to the next example\n",
    "# #                 true_start_onset = start_onset\n",
    "# #                 true_end_onset = end_onset  # save onset for next word \n",
    "# #                 cur_sentence.extend(x[0])  # add the word to the cur_sentence\n",
    "                \n",
    "# #         n_bins = int(math.ceil((end_onset - start_onset) / bin_fs))  # calculate number of bins\n",
    "        \n",
    "# #         '''If this is the end of the sentence (perform augmentation here)'''\n",
    "# #         if ((not x[1] and  # if not speaker 1 (speaker switched) and\n",
    "# #              speaker and  # speaker switched and\n",
    "# #              len(cur_sentence) >= 2)  # the sentence length is >= 2\n",
    "# #             and n_bins):  # number of bins is non-zero\n",
    "# #             cur_sentence.append(vocab[end_token])  # end the current sentence\n",
    "# #             labels.append(cur_sentence)  # put the sentence in the label vector/list\n",
    "# #             word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32) \n",
    "# #             for i, f in enumerate(np.array_split(ecogs[start_onset:end_onset,:], n_bins, axis=0)):\n",
    "# #                 word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "# #             signals.append(word_signal)\n",
    "           \n",
    "# #             '''\n",
    "# #             # Data augmentation by shifts\n",
    "# #             for i, s in enumerate(aug_shift_fs):\n",
    "# #                 aug_start = start_onset + s\n",
    "# #                 if aug_start < 0 or aug_start > ecogs.shape[0]:\n",
    "# #                     continue\n",
    "# #                 aug_end = end_onset + s\n",
    "# #                 if aug_end < 0 or aug_end > ecogs.shape[0]:\n",
    "# #                     continue\n",
    "# #                 n_bins = int(math.ceil((aug_end - aug_start) / bin_fs))\n",
    "# #                 if n_bins > 0:\n",
    "# #                     word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32)\n",
    "# #                     for i, f in enumerate(np.array_split(ecogs[aug_start:aug_end,:], n_bins, axis=0)):\n",
    "# #                         word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "# #                     signals.append(word_signal)\n",
    "# #                     labels.append(cur_sentence)\n",
    "# #             cur_sentence = [vocab[begin_token]]\n",
    "# #             '''\n",
    "# #         speaker = example[1]\n",
    "            \n",
    "#     ###################################################################    \n",
    "\n",
    "#     for x in examples:  # loop over each example\n",
    "#         if x[1]:  # Check if it is speaker 1\n",
    "#             if len(cur_sentence) == 1:  # if sentence length is 1 (that's the begin token)\n",
    "#                 start_onset = int(float(x[2])) + start_offset  # take the onset of that word\n",
    "#                 if start_onset < 0 or start_onset > ecogs.shape[0]:  # check if onset is within limits\n",
    "#                     continue  # move on to the next example\n",
    "#             new_end_onset = int(float(x[2])) + end_offset\n",
    "#             if (new_end_onset < 0 or  # if the window_end is less than 0 or \n",
    "#                 new_end_onset > ecogs.shape[0] or  # if the window_end is outside the signal \n",
    "#                 new_end_onset - start_onset < window_fs):  # if there are not enough frames in the window\n",
    "#                 continue  # move on to the next example\n",
    "#             end_onset = new_end_onset\n",
    "#             cur_sentence.extend(x[0])\n",
    "        \n",
    "#         n_bins = int(math.ceil((end_onset - start_onset) / bin_fs))  # calculate number of bins\n",
    "        \n",
    "#         if ((not x[1] and  # if not speaker 1 (speaker switched) and\n",
    "#              speaker and  # speaker switched and\n",
    "#              len(cur_sentence) >= 2)  # the sentence length is >= 2 and \n",
    "#             and (n_bins > 0)):  # number of bins is non-zero\n",
    "#             cur_sentence.append(vocab[end_token])  # end the current sentence\n",
    "#             labels.append(cur_sentence)  # put the sentence in the label vector/list\n",
    "#             max_len = max(max_len, len(cur_sentence))  # this is not really used anywhere\n",
    "#             word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32) \n",
    "#             for i, f in enumerate(np.array_split(ecogs[start_onset:end_onset,:], n_bins, axis=0)):\n",
    "#                 word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "#             signals.append(word_signal)\n",
    "          \n",
    "#             # Data augmentation by shifts\n",
    "#             for i, s in enumerate(aug_shift_fs):\n",
    "#                 aug_start = start_onset + s\n",
    "#                 if aug_start < 0 or aug_start > ecogs.shape[0]:\n",
    "#                     continue\n",
    "#                 aug_end = end_onset + s\n",
    "#                 if aug_end < 0 or aug_end > ecogs.shape[0]:\n",
    "#                     continue\n",
    "#                 n_bins = int(math.ceil((aug_end - aug_start) / bin_fs))\n",
    "#                 if n_bins > 0:\n",
    "#                     word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32)\n",
    "#                     for i, f in enumerate(np.array_split(ecogs[aug_start:aug_end,:], n_bins, axis=0)):\n",
    "#                         word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "#                     signals.append(word_signal)\n",
    "#                     labels.append(cur_sentence)\n",
    "#             cur_sentence = [vocab[begin_token]]\n",
    "#         speaker = x[1]\n",
    "    \n",
    "#     # what is this block of code doing here\n",
    "#     if speaker and len(cur_sentence) >= 2 and n_bins > 0:\n",
    "#         cur_sentence.append(vocab[end_token])\n",
    "#         labels.append(cur_sentence)\n",
    "#         n_bins = int(math.ceil((end_onset - start_onset) / bin_fs))\n",
    "#         word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32)\n",
    "#         for i, f in enumerate(np.array_split(ecogs[start_onset:end_onset,:], n_bins, axis=0)):\n",
    "#             word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "#         signals.append(word_signal)\n",
    "\n",
    "# if len(signals) == old_size:\n",
    "#     print(f'[WARNING] no examples built for {conversation}')\n",
    "\n",
    "# if len(signals) == 0:\n",
    "#     print('[ERROR] signals is empty')\n",
    "#     sys.exit(1)\n",
    "\n",
    "# print('final')\n",
    "# x = signals\n",
    "# y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for conversation, suffix, idx in convs[0:1]:\n",
    "    \n",
    "#     # Check if files exists, if it doesn't go to next\n",
    "#     datum_fn = glob.glob(conversation + suffix)[0]\n",
    "#     if not datum_fn:\n",
    "#         print('File DNE: ', conversation + suffix)\n",
    "#         continue\n",
    "\n",
    "#     # Extract electrode data\n",
    "#     ecogs = return_electrode_array(conversation, suffix)\n",
    "#     if not ecogs.size:\n",
    "#         print(f'Skipping bad conversation: {conversation}')\n",
    "#         continue\n",
    "\n",
    "#     # Read conversations and form examples\n",
    "#     old_size = len(signals)\n",
    "#     max_len = 0\n",
    "#     speaker = False\n",
    "#     cur_sentence, start_onset, end_onset = [w2i[begin_token]], 0, 0\n",
    "    \n",
    "#     examples = return_examples(datum_fn, delimiter, w2i, oov_token)\n",
    "#     examples1 = return_examples1(datum_fn, delimiter, w2i, oov_token)\n",
    "        \n",
    "#     for x in examples:\n",
    "#         if x[1]:  # first example in the list\n",
    "#             if len(cur_sentence) == 1:\n",
    "#                 start_onset = int(float(x[2])) + start_offset\n",
    "#                 if start_onset < 0 or start_onset > ecogs.shape[0]:\n",
    "#                     continue\n",
    "#             new_end_onset = int(float(x[2])) + end_offset\n",
    "#             if new_end_onset < 0 or new_end_onset > ecogs.shape[0]\\\n",
    "#                or new_end_onset - start_onset < window_fs:\n",
    "#                 continue\n",
    "#             end_onset = new_end_onset\n",
    "#             cur_sentence.append(x[0])\n",
    "        \n",
    "#         n_bins = int(math.ceil((end_onset - start_onset) / bin_fs))\n",
    "        \n",
    "#         if (((not x[1]) and speaker and len(cur_sentence) >= 2) or (x[1] and len(cur_sentence) >= 3)) \\\n",
    "#            and n_bins > 0:\n",
    "#             cur_sentence.append(w2i[end_token])\n",
    "#             labels.append(cur_sentence)\n",
    "#             max_len = max(max_len, len(cur_sentence))\n",
    "#             word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32)\n",
    "#             for i, f in enumerate(np.array_split(ecogs[start_onset:end_onset,:], n_bins, axis=0)):\n",
    "#                 word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "#             signals.append(word_signal)\n",
    "\n",
    "#             # Data augmentation by shifts\n",
    "#             for i, s in enumerate(aug_shift_fs):\n",
    "#                 aug_start = start_onset + s\n",
    "#                 if aug_start < 0 or aug_start > ecogs.shape[0]:\n",
    "#                     continue\n",
    "#                 aug_end = end_onset + s\n",
    "#                 if aug_end < 0 or aug_end > ecogs.shape[0]:\n",
    "#                     continue\n",
    "#                 n_bins = int(math.ceil((aug_end - aug_start) / bin_fs))\n",
    "#                 if n_bins > 0:\n",
    "#                     word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32)\n",
    "#                     for i, f in enumerate(np.array_split(ecogs[aug_start:aug_end,:], n_bins, axis=0)):\n",
    "#                         word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "#                     signals.append(word_signal)\n",
    "#                     labels.append(cur_sentence)\n",
    "#             cur_sentence = [w2i[begin_token]]\n",
    "#         speaker = x[1]\n",
    "    \n",
    "#     if speaker and len(cur_sentence) >= 2 and n_bins > 0:\n",
    "#         cur_sentence.append(w2i[end_token])\n",
    "#         labels.append(cur_sentence)\n",
    "#         n_bins = int(math.ceil((end_onset - start_onset) / bin_fs))\n",
    "#         word_signal = np.zeros((n_bins, len(electrodes)*len(subjects)), np.float32)\n",
    "#         for i, f in enumerate(np.array_split(ecogs[start_onset:end_onset,:], n_bins, axis=0)):\n",
    "#             word_signal[i,idx*len(electrodes):(idx+1)*len(electrodes)] = f.mean(axis=0)\n",
    "#         signals.append(word_signal)\n",
    "\n",
    "#     if len(signals) == old_size:\n",
    "#         print(f'[WARNING] no examples built for {conversation}')\n",
    "\n",
    "# if len(signals) == 0:\n",
    "#     print('[ERROR] signals is empty')\n",
    "#     sys.exit(1)\n",
    "\n",
    "# print('final')\n",
    "# x = signals\n",
    "# y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================================\n",
    "\n",
    "==========================================================================================================\n",
    "\n",
    "==========================================================================================================\n",
    "\n",
    "=========================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Default models and parameters\n",
    "# DEFAULT_MODELS = {\n",
    "#     \"ConvNet10\": (n_classes, ),\n",
    "#     \"PITOM\": (n_classes, len(args.electrodes) * len(args.subjects)),\n",
    "#     \"BrainClassifier\":\n",
    "#     (len(args.electrodes) * len(args.subjects), n_classes, args.tf_dmodel,\n",
    "#      args.tf_nhead, args.tf_nlayer, args.tf_dff, args.tf_dropout),\n",
    "#     \"BrainTransformer\":\n",
    "#     (len(args.electrodes) * len(args.subjects), n_classes, args.tf_dmodel,\n",
    "#      args.tf_nhead, args.tf_nlayer, args.tf_dff, args.tf_dropout)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import *\n",
    "\n",
    "# # Create model\n",
    "# if args.init_model is None:\n",
    "#     if args.model in DEFAULT_MODELS:\n",
    "#         print(\"Building default model: %s\" % args.model, end=\"\")\n",
    "#         model_class = globals()[args.model]\n",
    "#         model = model_class(*(DEFAULT_MODELS[args.model]))\n",
    "#     else:\n",
    "#         print(\"Building custom model: %s\" % args.model, end=\"\")\n",
    "#         sys.exit(1)\n",
    "# else:\n",
    "#     model_name = \"%s%s.pt\" % (SAVE_DIR, args.model)\n",
    "#     if os.path.isfile(model_name):\n",
    "#         model = torch.load(model_name)\n",
    "#         model = model.module if hasattr(model, 'module') else model\n",
    "#         print(\"Loaded initial model: %s \" % args.model)\n",
    "#     else:\n",
    "#         print(\"No models found in: \", SAVE_DIR)\n",
    "#         sys.exit(1)\n",
    "# print(\" with %d trainable parameters\" %\n",
    "#       sum([p.numel() for p in model.parameters() if p.requires_grad]))\n",
    "\n",
    "# sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# print(model)\n",
    "# for param in model.parameters():\n",
    "#     print(param.shape)\n",
    "# summary(model, x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "\n",
    "# # Initialize loss and optimizer\n",
    "# # weights = torch.ones(n_classes)\n",
    "# # max_freq = -1.\n",
    "# # for i in range(n_classes):\n",
    "# #     max_freq = max(max_freq, word2freq[vocab[i]])\n",
    "# #     weights[i] = 1./float(word2freq[vocab[i]])\n",
    "# # weights = weights*max_freq\n",
    "# # print(sorted([(vocab[i], round(float(weights[i]),1))\n",
    "# #               for i in range(n_classes)], key=lambda x: x[1]))\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # step_size = int(math.ceil(len(train_ds)/args.batch_size))\n",
    "# # optimizer = optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.98),\n",
    "# #                         eps=1e-9, weight_decay=args.weight_decay)\n",
    "# # optimizer = optim.AdamW(model.parameters(), lr=args.lr,\n",
    "# #                         weight_decay=args.weight_decay)\n",
    "# optimizer = AdamW(model.parameters(),\n",
    "#                   lr=args.lr,\n",
    "#                   weight_decay=args.weight_decay)\n",
    "# # optimizer = NoamOpt(args.tf_dmodel, 1., 2000,\n",
    "# #                     optim.Adam(model.parameters(),\n",
    "# #                                lr=0.,\n",
    "# #                                betas=(0.9, 0.98),\n",
    "# #                                eps=1e-9))\n",
    "# # scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "# #                                            milestones=[20*step_size,40*step_size],\n",
    "# #                                            gamma=0.2)\n",
    "# # scheduler = get_cosine_schedule_with_warmup(optimizer, 10*step_size,\n",
    "# #                                             args.epochs*step_size,\n",
    "# # num_cycles=0.5)\n",
    "# scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Move model and loss to GPUs\n",
    "# if args.gpus:\n",
    "#     if args.gpus > 1:\n",
    "#         model = nn.DataParallel(model)\n",
    "#     model.to(DEVICE)\n",
    "#     criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training on %d GPU(s) with batch_size %d for %d epochs\" %\n",
    "#       (args.gpus, args.batch_size, args.epochs))\n",
    "# print(\"=\" * CONFIG[\"print_pad\"])\n",
    "# sys.stdout.flush()\n",
    "\n",
    "# best_val_loss = float(\"inf\")\n",
    "# best_model = model\n",
    "# history = {\n",
    "#     'train_loss': [],\n",
    "#     'train_acc': [],\n",
    "#     'valid_loss': [],\n",
    "#     'valid_acc': []\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train_eval import SimpleLossCompute\n",
    "\n",
    "# train_loss_compute = SimpleLossCompute(criterion,\n",
    "#                                        opt=optimizer,\n",
    "#                                        scheduler=scheduler)\n",
    "# valid_loss_compute = SimpleLossCompute(criterion, opt=None, scheduler=None)\n",
    "\n",
    "# epoch = 0\n",
    "# model_name = \"%s%s.pt\" % (SAVE_DIR, args.model)\n",
    "# # Run training and validation for args.epochs epochs\n",
    "# lr = args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import warnings\n",
    "# from train_eval import train, valid\n",
    "\n",
    "# for epoch in range(1, args.epochs + 1):\n",
    "#     epoch_start_time = time.time()\n",
    "#     print('| train | epoch %d | ' % epoch, end='')\n",
    "#     train_loss, train_acc = train(train_dl, model, DEVICE,\n",
    "#                                   train_loss_compute, epoch, i2w)\n",
    "#     history['train_loss'].append(train_loss)\n",
    "#     history['train_acc'].append(train_acc)\n",
    "#     print('| valid | epoch %d | ' % epoch, end='')\n",
    "#     valid_loss, valid_acc = valid(valid_dl, model, DEVICE,\n",
    "#                                   valid_loss_compute, epoch)\n",
    "#     history['valid_loss'].append(valid_loss)\n",
    "#     history['valid_acc'].append(valid_acc)\n",
    "#     print('|' + '-' * (CONFIG[\"print_pad\"] - 2) + '|')\n",
    "#     # Store best model so far\n",
    "#     if valid_loss < best_val_loss:\n",
    "#         best_model, best_val_loss = model, valid_loss\n",
    "#         with warnings.catch_warnings():\n",
    "#             warnings.simplefilter(\"ignore\")\n",
    "#             model_to_save = best_model.module\\\n",
    "#                 if hasattr(best_model, 'module') else best_model\n",
    "#             torch.save(model_to_save, model_name)\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train_eval import plot_training\n",
    "\n",
    "# # Plot loss,accuracy vs. time and save figures\n",
    "# plot_training(history, SAVE_DIR, title=\"%s_lr%s\" % (args.model, args.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluating predictions on test set\")\n",
    "# # Load best model\n",
    "# model = torch.load(model_name)\n",
    "# if args.gpus:\n",
    "#     if args.gpus > 1:\n",
    "#         model = nn.DataParallel(model)\n",
    "#     model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start, end = 0, 0\n",
    "# softmax = nn.Softmax(dim=1)\n",
    "# all_preds = np.zeros((x_valid.size(0), n_classes), dtype=np.float32)\n",
    "# print('Allocating', np.prod(all_preds.shape) * 5 / 1e9, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate all predictions on test set\n",
    "# with torch.no_grad():\n",
    "#     for batch in valid_dl:\n",
    "#         src, trg = batch[0].to(DEVICE), batch[1].to(DEVICE,\n",
    "#                                                     dtype=torch.long)\n",
    "#         end = start + src.size(0)\n",
    "#         out = softmax(model(src))\n",
    "#         all_preds[start:end, :] = out.cpu()\n",
    "#         start = end\n",
    "\n",
    "# print(\"Calculated predictions\")\n",
    "# sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # Make categorical\n",
    "# n_examples = y_valid.shape[0]\n",
    "# categorical = np.zeros((n_examples, n_classes), dtype=np.float32)\n",
    "# categorical[np.arange(n_examples), y_valid] = 1\n",
    "\n",
    "# train_freq = Counter(y_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = all_preds\n",
    "# labels = y_valid.numpy()\n",
    "# train_freqs = train_freq\n",
    "# min_train=args.vocab_min_freq\n",
    "# suffix='-val'\n",
    "# prefix = ''\n",
    "# save_dir = SAVE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks = []\n",
    "# n_examples, n_classes = predictions.shape\n",
    "# fid = open(save_dir + 'guesses%s.csv' % suffix, 'w')\n",
    "# top1_uw, top5_uw, top10_uw = set(), set(), set()\n",
    "# accs, sizes = {}, [0.] * n_classes\n",
    "# total_freqs = float(sum(train_freqs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Go through each example and calculate its rank and top-k\n",
    "# for i in range(n_examples):\n",
    "#     y_true_idx = labels[i]\n",
    "#     word = i2w[y_true_idx]\n",
    "\n",
    "#     if train_freqs[y_true_idx] < min_train:\n",
    "#         continue\n",
    "\n",
    "#     # Get example predictions\n",
    "#     ex_preds = np.argsort(predictions[i])[::-1]\n",
    "#     rank = np.where(y_true_idx == ex_preds)[0][0]\n",
    "#     ranks.append(rank)\n",
    "\n",
    "#     if i == 0:\n",
    "#         raise Exception('Done')\n",
    "#     fid.write('%s,%d,' % (word, rank))\n",
    "#     fid.write(','.join(i2w[j] for j in ex_preds[:10]))\n",
    "#     fid.write('\\n')\n",
    "\n",
    "#     if rank == 0:\n",
    "#         top1_uw.add(ex_preds[0])\n",
    "#     elif rank < 5:\n",
    "#         top5_uw.update(ex_preds[:5])\n",
    "#     elif rank < 10:\n",
    "#         top10_uw.update(ex_preds[:10])\n",
    "\n",
    "#     # Counts of how many times a word has been ranked the top\n",
    "#     accs[i2w[y_true_idx]] = accs.get(i2w[y_true_idx], 0.) + (rank == 0)\n",
    "    \n",
    "#     # size of validation set excluding the samples with < min_freq\n",
    "#     sizes[y_true_idx] += 1.\n",
    "\n",
    "# print(accs, sizes, n_examples, sum(sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(n_classes):\n",
    "#     chance_acc = train_freqs[i] / total_freqs * 100.\n",
    "#     if sizes[i] > 0:\n",
    "#         rounded_acc = round(accs[i2w[i]] / sizes[i] * 100, 3)\n",
    "#         accs[i2w[i]] = (rounded_acc, chance_acc, rounded_acc - chance_acc)\n",
    "#     else:\n",
    "#         accs[i2w[i]] = (0., chance_acc, -chance_acc)\n",
    "        \n",
    "# accs = sorted(accs.items(), key=lambda x: -x[1][2])\n",
    "# fid.close()\n",
    "# print('Top1 #Unique:', len(top1_uw))\n",
    "# print('Top5 #Unique:', len(top5_uw))\n",
    "# print('Top10 #Unique:', len(top10_uw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_examples = len(ranks)\n",
    "# ranks = np.array(ranks)\n",
    "# top1 = sum(ranks == 0) / (1e-12 + len(ranks)) * 100\n",
    "# top5 = sum(ranks < 5) / (1e-12 + len(ranks)) * 100\n",
    "# top10 = sum(ranks < 10) / (1e-12 + len(ranks)) * 100\n",
    "\n",
    "# print(top1, top5, top10)\n",
    "# # Calculate chance levels based on training word frequencies\n",
    "# freqs = Counter(labels)\n",
    "# print(freqs, len(labels))\n",
    "# freqs = np.array([freqs[i] for i, _ in train_freqs.most_common()])\n",
    "# print(freqs)\n",
    "# freqs = freqs[freqs != 0]\n",
    "# print(freqs)\n",
    "# chances = (freqs / freqs.sum()).cumsum() * 100\n",
    "\n",
    "# # Print and write to file\n",
    "# if suffix is not None:\n",
    "#     with open(save_dir + 'topk%s.txt' % suffix, 'w') as fout:\n",
    "#         line = 'n_classes: %d\\nn_examples: %d' % (n_classes, n_examples)\n",
    "#         print(line)\n",
    "#         fout.write(line + '\\n')\n",
    "#         line = 'Top-1\\t%.4f %% (%.2f %%)' % (top1, chances[0])\n",
    "#         print(line)\n",
    "#         fout.write(line + '\\n')\n",
    "#         line = 'Top-5\\t%.4f %% (%.2f %%)' % (top5, chances[4])\n",
    "#         print(line)\n",
    "#         fout.write(line + '\\n')\n",
    "#         line = 'Top-10\\t%.4f %% (%.2f %%)' % (top10, chances[9])\n",
    "#         print(line)\n",
    "#         fout.write(line + '\\n')\n",
    "#         line = 'Token Accuracies\\t%s' % str(accs)\n",
    "#         print(line)\n",
    "#         fout.write(line + '\\n')\n",
    "\n",
    "# final_dict = {\n",
    "#     prefix + 'top1': top1,\n",
    "#     prefix + 'top5': top5,\n",
    "#     prefix + 'top10': top10,\n",
    "#     prefix + 'top1_chance': chances[0],\n",
    "#     prefix + 'top5_chance': chances[4],\n",
    "#     prefix + 'top10_chance': chances[9],\n",
    "#     prefix + 'top1_above': (top1 - chances[0]) / chances[0],\n",
    "#     prefix + 'top5_above': (top5 - chances[4]) / chances[4],\n",
    "#     prefix + 'top10_above': (top10 - chances[9]) / chances[9],\n",
    "#     prefix + 'top1_n_uniq_correct': len(top1_uw),\n",
    "#     prefix + 'top5_n_uniq_correct': len(top5_uw),\n",
    "#     prefix + 'top10_n_uniq_correct': len(top10_uw),\n",
    "#     prefix + 'word_accuracies': accs\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate ROC-AUC\n",
    "# print(\"Evaluating ROC-AUC\")\n",
    "# sys.stdout.flush()\n",
    "# res.update(\n",
    "#     evaluate_roc(all_preds,\n",
    "#                  categorical,\n",
    "#                  i2w,\n",
    "#                  train_freq,\n",
    "#                  SAVE_DIR,\n",
    "#                  do_plot=not args.no_plot,\n",
    "#                  min_train=args.vocab_min_freq))\n",
    "# pprint(res.items())\n",
    "# print(\"Saving results\")\n",
    "# with open(SAVE_DIR + \"results.json\", \"w\") as fp:\n",
    "#     json.dump(res, fp, indent=4)\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate ROC performance of the model\n",
    "# # (predictions, labels of shape (n_examples, n_classes))\n",
    "# predictions = all_preds\n",
    "# labels = categorical\n",
    "# train_freqs = train_freq\n",
    "# save_dir = SAVE_DIR\n",
    "# do_plot = not args.no_plot\n",
    "# given_thresholds=None\n",
    "# title=''\n",
    "# suffix=''\n",
    "# min_train=args.vocab_min_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert (predictions.shape == labels.shape)\n",
    "# lines, scores, word_freqs = [], [], []\n",
    "# n_examples, n_classes = predictions.shape\n",
    "# thresholds = np.full(n_classes, np.nan)\n",
    "# rocs, fprs, tprs = {}, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create directory for plots if required\n",
    "# if do_plot:\n",
    "#     roc_dir = save_dir + 'rocs/'\n",
    "#     if not os.path.isdir(roc_dir):\n",
    "#         os.mkdir(roc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import auc, confusion_matrix, roc_curve\n",
    "# from train_eval import best_threshold\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Go over each class and compute AUC\n",
    "# for i in range(n_classes):\n",
    "#     train_count = train_freqs[i]\n",
    "#     n_true = np.count_nonzero(labels[:, i])\n",
    "#     if train_count < min_train or n_true == 0:\n",
    "#         continue\n",
    "#     word = i2w[i]\n",
    "#     probs = predictions[:, i]\n",
    "#     c_labels = labels[:, i]\n",
    "#     fpr, tpr, thresh = roc_curve(c_labels, probs)\n",
    "#     if given_thresholds is None:\n",
    "#         x, y, threshold = best_threshold(fpr, tpr, thresh)\n",
    "#     else:\n",
    "#         x, y, threshold = 0, 0, given_thresholds[i]\n",
    "#     thresholds[i] = threshold\n",
    "#     score = auc(fpr, tpr)\n",
    "#     scores.append(score)\n",
    "#     word_freqs.append(train_count)\n",
    "#     rocs[word] = score\n",
    "#     fprs.append(fpr)\n",
    "#     tprs.append(tpr)\n",
    "#     y_pred = probs >= threshold\n",
    "#     tn, fp, fn, tp = confusion_matrix(c_labels, y_pred).ravel()\n",
    "#     lines.append('%s\\t%3d\\t%3d\\t%.3f\\t%d\\t%d\\t%d\\t%d\\n' %\n",
    "#                  (word, n_true, train_count, score, tp, fp, fn, tn))\n",
    "#     if do_plot:\n",
    "#         fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "#         axes[0].plot(fpr, tpr, color='darkorange', lw=2, marker='.')\n",
    "#         axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "#         axes[0].plot(x, y, marker='o', color='blue')\n",
    "#         axes[0].set_xlim([0.0, 1.0])\n",
    "#         axes[0].set_ylim([0.0, 1.05])\n",
    "#         axes[0].set_xlabel('False Positive Rate')\n",
    "#         axes[0].set_ylabel('True Positive Rate')\n",
    "#         h1 = probs[c_labels == 1].reshape(-1)\n",
    "#         h2 = probs[c_labels == 0].reshape(-1)\n",
    "#         axes[1].hist(h2,\n",
    "#                      bins=20,\n",
    "#                      color='orange',\n",
    "#                      alpha=0.5,\n",
    "#                      label='Neg. Examples')\n",
    "#         # axes[1].twinx().hist(h1, bins=50, alpha=0.5,\n",
    "#         # label='Pos. Examples')\n",
    "#         axes[1].hist(h1, bins=50, alpha=0.5, label='Pos. Examples')\n",
    "#         axes[1].axvline(threshold, color='k')\n",
    "#         axes[1].set_xlabel('Activation')\n",
    "#         axes[1].set_ylabel('Frequency')\n",
    "#         axes[1].legend()\n",
    "#         axes[1].set_title('%d TP | %d FP | %d FN | %d TN' %\n",
    "#                           (tp, fp, fn, tn))\n",
    "#         fig.suptitle('ROC Curve | %s | AUC = %.3f | N = %d' %\n",
    "#                      (word, score, n_true))\n",
    "#         plt.savefig(roc_dir + '%s.png' % word)\n",
    "#         fig.clear()\n",
    "#         plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute statistics\n",
    "# scores, word_freqs = np.array(scores), np.array(word_freqs)\n",
    "# normed_freqs = word_freqs / word_freqs.sum()\n",
    "# avg_auc = scores.mean()\n",
    "# weighted_avg = (scores * normed_freqs).sum()\n",
    "# print('Avg AUC: %d\\t%.6f' % (scores.size, avg_auc))\n",
    "# print('Weighted Avg AUC: %d\\t%.6f' % (scores.size, weighted_avg))\n",
    "\n",
    "# # Write to file\n",
    "# with open(save_dir + 'aucs%s.txt' % suffix, 'w') as fout:\n",
    "#     for line in lines:\n",
    "#         fout.write(line)\n",
    "\n",
    "# # Plot histogram and AUC as a function of num of examples\n",
    "# _, ax = plt.subplots(1, 1)\n",
    "# ax.scatter(word_freqs, scores, marker='.')\n",
    "# ax.set_xlabel('# examples')\n",
    "# ax.set_ylabel('AUC')\n",
    "# ax.set_title('%s | avg: %.3f | N = %d' %\n",
    "#              (title, weighted_avg, scores.size))\n",
    "# ax.set_yticks(np.arange(0., 1.1, 0.1))\n",
    "# ax.grid()\n",
    "# plt.savefig(save_dir + 'roc-auc-examples.png', bbox_inches='tight')\n",
    "\n",
    "# _, ax = plt.subplots(1, 1)\n",
    "# ax.hist(scores, bins=20)\n",
    "# ax.set_xlabel('AUC')\n",
    "# ax.set_ylabel('# labels')\n",
    "# ax.set_title('%s | avg: %.3f | N = %d' %\n",
    "#              (title, weighted_avg, scores.size))\n",
    "# ax.set_xticks(np.arange(0., 1., 0.1))\n",
    "# plt.savefig(save_dir + 'roc-auc.png', bbox_inches='tight')\n",
    "\n",
    "# _, ax = plt.subplots(1, 1)\n",
    "# for fpr, tpr in zip(fprs, tprs):\n",
    "#     ax.plot(fpr, tpr, lw=1)\n",
    "# ax.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "# ax.set_xlim([0.0, 1.0])\n",
    "# ax.set_ylim([0.0, 1.05])\n",
    "# ax.set_xlabel('False Positive Rate')\n",
    "# ax.set_ylabel('True Positive Rate')\n",
    "# ax.set_title('%s | avg: %.3f | N = %d' %\n",
    "#              (title, weighted_avg, scores.size))\n",
    "# plt.savefig(save_dir + 'roc-auc-all.png', bbox_inches='tight')\n",
    "\n",
    "# final_dict.update({\n",
    "#     'rocauc_avg': avg_auc,\n",
    "#     'rocauc_stddev': scores.std(),\n",
    "#     'rocauc_w_avg': weighted_avg,\n",
    "#     'rocauc_n': scores.size,\n",
    "#     'rocs': rocs\n",
    "# })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
