{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Seq2Seq (Neural Signals to Bi-grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for running a 2-word seq2seq Transformer where the neural signals are sent through the encoder while the corresponding bi-grams are sent through the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the seed for reproducibility. For more info read https://pytorch.org/docs/stable/notes/randomness.html and https://discuss.pytorch.org/t/random-seed-initialization/7854/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from transformers import AdamW\n",
    "\n",
    "from arg_parser import arg_parser\n",
    "from build_matrices import (build_design_matrices_classification,\n",
    "                            build_design_matrices_seq2seq)\n",
    "from config import build_config\n",
    "from dl_utils import Brain2enDataset, MyCollator\n",
    "from models import PITOM, ConvNet10, MeNTAL, MeNTALmini\n",
    "from train_eval import plot_training, train, valid\n",
    "from eval_utils import evaluate_roc, evaluate_topk\n",
    "from vocab_builder import get_sp_vocab, get_std_vocab, get_vocab\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = '20200531-ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arg_parser(['--subjects', '625'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model objectives\n",
    "MODEL_OBJ = {\n",
    "    \"ConvNet10\": \"classifier\",\n",
    "    \"PITOM\": \"classifier\",\n",
    "    \"MeNTALmini\": \"classifier\",\n",
    "    \"MeNTAL\": \"seq2seq\"\n",
    "}\n",
    "\n",
    "# GPUs\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "args.gpus = min(args.gpus, torch.cuda.device_count())\n",
    "\n",
    "# Fix random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "args.model = args.model.split(\"_\")[0]\n",
    "classify = False if (args.model in MODEL_OBJ\n",
    "                     and MODEL_OBJ[args.model] == \"seq2seq\") else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 625\n",
      "Training Data:: Number of Conversations is: 63\n",
      "Validation Data:: Number of Conversations is: 13\n"
     ]
    }
   ],
   "source": [
    "CONFIG = build_config(args, results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2freq, word_list, n_classes, vocab, i2w = get_std_vocab(\n",
    "    CONFIG, comprehension=False, classify=classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "\n",
    "def figure1(SAVE_DIR, word2freq):\n",
    "    '''Plotting histogram of word frequency'''\n",
    "    try:\n",
    "        k = list(word2freq.values())\n",
    "    except:\n",
    "        k = word2freq\n",
    "    k = list(filter((-1).__ne__, k))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(len(k)), sorted(k))\n",
    "    ax.xaxis.set_major_formatter(mtick.PercentFormatter(len(word2freq)))\n",
    "    plt.title('Frequency of words (sorted)', fontsize=16)\n",
    "    plt.xlabel('Percentage of Words', fontsize=16)\n",
    "    plt.ylabel('Word Frequency', fontsize=16)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, which='both')\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'word_frequency.svg'))\n",
    "    plt.show(fig)\n",
    "\n",
    "\n",
    "def figure2(SAVE_DIR, word2freq):\n",
    "    bins = [0, 5, 10, 20, 30, 40, 50, 100, 250, 500, 750, 1000, 5000]\n",
    "    try:\n",
    "        k = list(word2freq.values())\n",
    "    except:\n",
    "        k = word2freq\n",
    "    k = list(filter((-1).__ne__, k))\n",
    "    categories = pd.cut(k, bins)\n",
    "    price_binned = pd.value_counts(categories).reindex(categories.categories)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(range(0, len(bins)-1), price_binned, width=1, align='edge')\n",
    "    plt.xticks(range(len(bins)), labels=bins, rotation='45')\n",
    "\n",
    "    for i, v in enumerate(price_binned.values):\n",
    "        ax.text(i + 0.25, v + 5, str(v), color='blue', fontweight='bold')\n",
    "    \n",
    "    plt.title('Distribution of Words Frequency', fontsize=16)\n",
    "    plt.xlabel('Word Frequency', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'word_frequency_dist.svg'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1(CONFIG[\"SAVE_DIR\"], word2freq)\n",
    "figure2(CONFIG[\"SAVE_DIR\"], word2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arg_parser(['--subjects', '625',\n",
    "                   '--max-electrodes', '55',\n",
    "                   '--vocab-min-freq', '10',\n",
    "                   '--vocab-max-freq', '250',\n",
    "                  '--epochs', '5'])\n",
    "CONFIG = build_config(args, results_folder)\n",
    "args.gpus = min(args.gpus, torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2freq, word_list, n_classes, vocab, i2w = get_std_vocab(\n",
    "    CONFIG, comprehension=False, classify=classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1(CONFIG[\"SAVE_DIR\"], word2freq)\n",
    "figure2(CONFIG[\"SAVE_DIR\"], word2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training data\")\n",
    "x_train, y_train = build_design_matrices_seq2seq(\n",
    "    'train', CONFIG, vocab, delimiter=\" \", aug_shift_ms=[-1000, -500])\n",
    "\n",
    "print(\"Loading validation data\")\n",
    "x_valid, y_valid = build_design_matrices_seq2seq(\n",
    "    'valid', CONFIG, vocab, delimiter=\" \", aug_shift_ms=[], remove_unks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some insights about the bigrams in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_len_stats(x_train, x_valid):\n",
    "    train_seq_lengths = [sample.shape[0] for sample in x_train]\n",
    "    valid_seq_lengths = [sample.shape[0] for sample in x_valid]\n",
    "    print(\"Training Seq Lengths::\")\n",
    "    print(f\"\\tMin: {min(train_seq_lengths)}\") \n",
    "    print(f\"\\tMax: {max(train_seq_lengths)}\")\n",
    "    print(f\"\\tMean: {np.mean(train_seq_lengths):.2f}\")\n",
    "    print(f\"\\tMedian: {np.median(train_seq_lengths):.2f}\")\n",
    "    print(f\"\\tStd: {np.std(train_seq_lengths):.2f}\")\n",
    "\n",
    "    print(\"Validation Seq Lengths::\")\n",
    "    print(f\"\\tMin: {min(valid_seq_lengths)}\") \n",
    "    print(f\"\\tMax: {max(valid_seq_lengths)}\")\n",
    "    print(f\"\\tMean: {np.mean(valid_seq_lengths):.2f}\")\n",
    "    print(f\"\\tMedian: {np.median(valid_seq_lengths):.2f}\")\n",
    "    print(f\"\\tStd: {np.std(valid_seq_lengths):.2f}\")\n",
    "    \n",
    "    return train_seq_lengths, valid_seq_lengths\n",
    "    \n",
    "def figure4(SAVE_DIR, lengths, string):\n",
    "    '''Plotting histogram of Training Signal Lengths'''\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(len(lengths)), sorted(lengths))\n",
    "    ax.xaxis.set_major_formatter(mtick.PercentFormatter(len(lengths)))\n",
    "    plt.title(string + ' set Seq lengths (sorted)', fontsize=16)\n",
    "    plt.xlabel('Percentage of Samples', fontsize=14)\n",
    "    plt.ylabel('Sequence Length', fontsize=14)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, which='both')\n",
    "    plt.savefig(os.path.join(SAVE_DIR, string + '_signal_len.svg'))\n",
    "    plt.show(fig)\n",
    "    \n",
    "\n",
    "def figure5(SAVE_DIR, lengths, string):\n",
    "    bins = [0, 25, 50, 75, 100, 250, 500, 1000, 2500, 5000, 7500, 10000]\n",
    "\n",
    "    categories = pd.cut(lengths, bins)\n",
    "    price_binned = pd.value_counts(categories).reindex(categories.categories)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(range(0, len(bins)-1), price_binned, width=1, align='edge')\n",
    "    plt.xticks(range(len(bins)), labels=bins)\n",
    "\n",
    "    for i, v in enumerate(price_binned.values):\n",
    "        ax.text(i + 0.25, v + 5, str(v), color='blue', fontweight='bold')\n",
    "    \n",
    "    plt.title(f'Distribution of Seq lengths ({string})', fontsize=14)\n",
    "    plt.xlabel('Sequence Length', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.savefig(os.path.join(SAVE_DIR, string + '_signal_len_dist.svg'))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def figure6(SAVE_DIR, lengths, string):\n",
    "    plt.hist(lengths, bins=1000)\n",
    "    plt.xlim([0, 100])\n",
    "    plt.title(f'Distribution of Seq lengths ({string})', fontsize=14)\n",
    "    plt.xlabel('Sequence Length', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.savefig(os.path.join(SAVE_DIR, string + '_signal_len_dist_zoom.svg'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_lengths, valid_seq_lengths = seq_len_stats(x_train, x_valid)\n",
    "    \n",
    "figure4(CONFIG[\"SAVE_DIR\"], train_seq_lengths, 'Training')\n",
    "figure4(CONFIG[\"SAVE_DIR\"], valid_seq_lengths, 'Validation')\n",
    "\n",
    "figure5(CONFIG[\"SAVE_DIR\"], train_seq_lengths, 'Training')\n",
    "figure5(CONFIG[\"SAVE_DIR\"], valid_seq_lengths, 'Validation')\n",
    "\n",
    "figure6(CONFIG[\"SAVE_DIR\"], train_seq_lengths, 'Training')\n",
    "figure6(CONFIG[\"SAVE_DIR\"], valid_seq_lengths, 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training data\")\n",
    "x_train, y_train = build_design_matrices_seq2seq(\n",
    "    'train', CONFIG, vocab, delimiter=\" \", aug_shift_ms=[-1000, -500], max_num_bins=60)\n",
    "\n",
    "# print(\"Loading validation data\")\n",
    "# x_valid, y_valid = build_design_matrices_seq2seq(\n",
    "#     'valid', CONFIG, vocab, delimiter=\" \", aug_shift_ms=[], max_num_bins=60, remove_unks=False)\n",
    "\n",
    "print(\"Loading validation data\")\n",
    "x_valid, y_valid = build_design_matrices_seq2seq(\n",
    "    'valid', CONFIG, vocab, delimiter=\" \", aug_shift_ms=[], max_num_bins=60, remove_unks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_lengths, valid_seq_lengths = seq_len_stats(x_train, x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(data):\n",
    "    df_y_train = pd.DataFrame(data)\n",
    "    df_y_train[1].replace(i2w, inplace=True)\n",
    "    df_y_train[2].replace(i2w, inplace=True)\n",
    "\n",
    "    return df_y_train\n",
    "\n",
    "\n",
    "def bigram_freq_excel(data, word2freq, i2w, filename, ref_data=None):\n",
    "    valid_df = replace_words(data)\n",
    "    valid_df = valid_df.groupby([1, 2]).size().reset_index(name='Count')\n",
    "    valid_df['BF1'] = valid_df[1].replace(dict(valid_df[1].value_counts()))\n",
    "    valid_df['BF2'] = valid_df[2].replace(dict(valid_df[2].value_counts()))\n",
    "    valid_df['VF1'] = valid_df[1].replace(word2freq)\n",
    "    valid_df['VF2'] = valid_df[2].replace(word2freq)\n",
    "\n",
    "    if ref_data is not None:\n",
    "        valid_df = valid_df.merge(ref_data, on=[1, 2], suffixes=('_valid', '_train'), how='left') \n",
    "        \n",
    "    valid_df.to_excel(os.path.join(CONFIG[\"SAVE_DIR\"], filename), index=False)\n",
    "        \n",
    "    print(len(valid_df[1].unique()))\n",
    "    print(len(valid_df[2].unique()))\n",
    "\n",
    "    print(set(word2freq.keys()) - set(valid_df[1].unique()))\n",
    "    print(set(word2freq.keys()) - set(valid_df[2].unique()))\n",
    "    \n",
    "    return valid_df\n",
    "\n",
    "\n",
    "raw_train_df = bigram_freq_excel(y_train, word2freq, i2w, \"625_bi-gram-freq-train.xlsx\")\n",
    "_ = bigram_freq_excel(y_valid, word2freq, i2w, \"625_bi-gram-freq-valid.xlsx\", ref_data=raw_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def figure6(SAVE_DIR, df, word2freq, string):\n",
    "#     sorted_w2f = sorted(word2freq.items())\n",
    "#     l = [a[1] for a in sorted_w2f if a[1] != -1]\n",
    "#     plt.plot(df[1].value_counts().sort_index(), marker='.', markersize = 2.5, linewidth=0.25)\n",
    "#     plt.plot(df[2].value_counts().sort_index(), marker='.', markersize = 2.5, linewidth=0.25)\n",
    "#     plt.plot(l, marker='.', markersize = 2.5, linewidth=0.25, color='k')\n",
    "#     plt.xticks(list(range(0, len(vocab), 50)), list(range(0, len(vocab), 50)))\n",
    "#     plt.legend(['First word', 'Second Word', 'Actual'])\n",
    "#     plt.xlabel('Word Index', fontsize=14)\n",
    "#     plt.ylabel('Frequency', fontsize=14)\n",
    "#     plt.yscale('log')\n",
    "#     plt.title(f'Frequency of each word in the bigram ({string})', fontsize=14)\n",
    "#     plt.savefig(os.path.join(SAVE_DIR, string + '_bigram-Freq.svg'))\n",
    "#     plt.show()\n",
    "    \n",
    "# figure6(SAVE_DIR, train_df, word2freq, 'Training')\n",
    "# figure6(SAVE_DIR, train_df, word2freq, 'Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting train and validation data to Loader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain2enDataset(Dataset):\n",
    "    \"\"\"Brainwave-to-English Dataset.\n",
    "       Pytorch Dataset wrapper\n",
    "    \"\"\"\n",
    "    def __init__(self, signals, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            signals (list): brainwave examples.\n",
    "            labels (list): english examples.\n",
    "        \"\"\"\n",
    "        # global oov_token, vocab\n",
    "\n",
    "        assert (len(signals) == len(labels))\n",
    "        indices = [(i, len(signals[i]), len(labels[i]))\n",
    "                   for i in range(len(signals))]\n",
    "        indices.sort(key=lambda x: (x[1], x[2], x[0]))\n",
    "        self.examples = []\n",
    "        self.max_seq_len = 0\n",
    "        self.max_sent_len = 0\n",
    "        self.train_freq = Counter()\n",
    "        c = 0\n",
    "        for i in indices:\n",
    "            if i[1] > 384 or i[2] < 4 or i[2] > 128:\n",
    "                c += 1\n",
    "                continue\n",
    "            lab = labels[i[0]]\n",
    "            self.train_freq.update(lab)\n",
    "            lab = torch.tensor(lab).long()\n",
    "            self.examples.append(\n",
    "                (torch.from_numpy(signals[i[0]]).float(), lab))\n",
    "            self.max_seq_len = max(self.max_seq_len, i[1])\n",
    "            self.max_sent_len = max(self.max_sent_len, len(lab))\n",
    "        print(\"Skipped\", c, \"examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Brain2enDataset(x_train, y_train)\n",
    "print(\"Number of training signals: \", len(train_ds))\n",
    "valid_ds = Brain2enDataset(x_valid, y_valid)\n",
    "print(\"Number of validation signals: \", len(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollator(object):\n",
    "    def __init__(self, CONFIG, vocabulary):\n",
    "        self.CONFIG = CONFIG\n",
    "        self.vocabulary = vocabulary\n",
    "        self.pad_token = CONFIG[\"pad_token\"]\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # do something with batch and self.params\n",
    "        src = pad_sequence([batch[i][0] for i in range(len(batch))],\n",
    "                           batch_first=True,\n",
    "                           padding_value=0.)\n",
    "        labels = pad_sequence([batch[i][1] for i in range(len(batch))],\n",
    "                              batch_first=True,\n",
    "                              padding_value=self.vocabulary[self.pad_token])\n",
    "        trg = torch.zeros(labels.size(0), labels.size(1),\n",
    "                          len(self.vocabulary)).scatter_(\n",
    "                              2, labels.unsqueeze(-1), 1)\n",
    "        trg, trg_y = trg[:, :-1, :], labels[:, 1:]\n",
    "        pos_mask, pad_mask = self.masks(trg_y)\n",
    "        return src, trg, trg_y, pos_mask, pad_mask\n",
    "\n",
    "    def masks(self, labels):\n",
    "        pos_mask = (torch.triu(torch.ones(labels.size(1),\n",
    "                                          labels.size(1))) == 1).transpose(\n",
    "                                              0, 1).unsqueeze(0)\n",
    "        pos_mask = pos_mask.float().masked_fill(pos_mask == 0,\n",
    "                                                float('-inf')).masked_fill(\n",
    "                                                    pos_mask == 1, float(0.0))\n",
    "        pad_mask = labels == self.vocabulary[self.pad_token]\n",
    "        return pos_mask, pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_ds[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG[\"pad_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = pad_sequence([batch[i][0] for i in range(len(batch))],\n",
    "                           batch_first=True,\n",
    "                           padding_value=0.)\n",
    "labels = pad_sequence([batch[i][1] for i in range(len(batch))],\n",
    "                              batch_first=True,\n",
    "                              padding_value=vocab[CONFIG[\"pad_token\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg = torch.zeros(labels.size(0), labels.size(1),\n",
    "                  len(vocab)).scatter_(\n",
    "                      2, labels.unsqueeze(-1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg, trg_y = trg[:, :-1, :], labels[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mask = (torch.triu(torch.ones(labels.size(1),\n",
    "                                  labels.size(1))) == 1).transpose(\n",
    "                                      0, 1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mask = pos_mask.float().masked_fill(pos_mask == 0,\n",
    "                                        float('-inf')).masked_fill(\n",
    "                                            pos_mask == 1, float(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask = labels == vocab[CONFIG[\"pad_token\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collator = MyCollator(CONFIG, vocab)\n",
    "train_dl = data.DataLoader(train_ds,\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=True,\n",
    "                           num_workers=CONFIG[\"num_cpus\"],\n",
    "                           collate_fn=my_collator)\n",
    "valid_dl = data.DataLoader(valid_ds,\n",
    "                           batch_size=args.batch_size,\n",
    "                           num_workers=CONFIG[\"num_cpus\"],\n",
    "                           collate_fn=my_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODELS = {\n",
    "    \"ConvNet10\": (len(vocab), ),\n",
    "    \"PITOM\": (len(vocab), sum(args.max_electrodes)),\n",
    "    \"MeNTALmini\":\n",
    "    (sum(args.max_electrodes), len(vocab), args.tf_dmodel, args.tf_nhead,\n",
    "     args.tf_nlayer, args.tf_dff, args.tf_dropout),\n",
    "    \"MeNTAL\": (sum(args.max_electrodes), len(vocab), args.tf_dmodel,\n",
    "               args.tf_nhead, args.tf_nlayer, args.tf_dff, args.tf_dropout)\n",
    "}\n",
    "\n",
    "# Create model\n",
    "if args.init_model is None:\n",
    "    if args.model in DEFAULT_MODELS:\n",
    "        print(\"Building default model: %s\" % args.model, end=\"\")\n",
    "        model_class = globals()[args.model]\n",
    "        model = model_class(*(DEFAULT_MODELS[args.model]))\n",
    "    else:\n",
    "        print(\"Building custom model: %s\" % args.model, end=\"\")\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    model_name = \"%s%s.pt\" % (SAVE_DIR, args.model)\n",
    "    if os.path.isfile(model_name):\n",
    "        model = torch.load(model_name)\n",
    "        model = model.module if hasattr(model, 'module') else model\n",
    "        print(\"Loaded initial model: %s \" % args.model)\n",
    "    else:\n",
    "        print(\"No models found in: \", SAVE_DIR)\n",
    "        sys.exit(1)\n",
    "print(\" with %d trainable parameters\" %\n",
    "      sum([p.numel() for p in model.parameters() if p.requires_grad]))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "step_size = int(math.ceil(len(train_ds) / args.batch_size))\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=args.lr,\n",
    "                  weight_decay=args.weight_decay)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training on %d GPU(s) with batch_size %d for %d epochs\" %\n",
    "      (args.gpus, args.batch_size, args.epochs))\n",
    "print(\"=\" * CONFIG[\"print_pad\"])\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = model\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'valid_loss': [],\n",
    "    'valid_acc': []\n",
    "}\n",
    "\"\"\" train_loss_compute = SimpleLossCompute(criterion,\n",
    "                                       opt=optimizer,\n",
    "                                       scheduler=scheduler)\n",
    "valid_loss_compute = SimpleLossCompute(criterion, opt=None, scheduler=None)\n",
    "\"\"\"\n",
    "epoch = 0\n",
    "model_name = \"%s%s.pt\" % (CONFIG[\"SAVE_DIR\"], args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG[\"SAVE_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "data_iter = train_dl\n",
    "device = DEVICE\n",
    "opt = optimizer\n",
    "scheduler = scheduler\n",
    "seq2seq = True\n",
    "pad_idx = vocab[CONFIG[\"pad_token\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = iter(data_iter)\n",
    "batch = next(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "start_time = time.time()\n",
    "total_loss = 0.\n",
    "total_acc = 0.\n",
    "count, batch_count = 0, 0\n",
    "CLIP_NORM = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent gradient accumulation\n",
    "model.zero_grad()\n",
    "src = batch[0].to(device)\n",
    "trg = batch[1].long().to(device)\n",
    "\n",
    "trg_y = batch[2].long().to(device)\n",
    "trg_pos_mask, trg_pad_mask = batch[3].to(device), batch[4].to(\n",
    "    device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform loss computation during forward pass for parallelism\n",
    "out, trg_y, loss = model.forward(src, trg, trg_pos_mask,\n",
    "                                 trg_pad_mask, trg_y, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (trg_y != pad_idx).nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss += loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = out[idx]\n",
    "trg_y1 = trg_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining on %d GPU(s) with batch_size %d for %d epochs\" %\n",
    "      (args.gpus, args.batch_size, args.epochs))\n",
    "sys.stdout.flush()\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model = model\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'valid_loss': [],\n",
    "    'valid_acc': []\n",
    "}\n",
    "\"\"\" train_loss_compute = SimpleLossCompute(criterion,\n",
    "                                       opt=optimizer,\n",
    "                                       scheduler=scheduler)\n",
    "valid_loss_compute = SimpleLossCompute(criterion, opt=None, scheduler=None)\n",
    "\"\"\"\n",
    "epoch = 0\n",
    "model_name = \"%s%s.pt\" % (CONFIG[\"SAVE_DIR\"], args.model)\n",
    "\"\"\" totalfreq = float(sum(train_ds.train_freq.values()))\n",
    "print(\n",
    "    sorted(((i2w[l], f / totalfreq)\n",
    "            for l, f in train_ds.train_freq.most_common()),\n",
    "           key=lambda x: -x[1]))\n",
    "\"\"\"\n",
    "# Run training and validation for args.epochs epochs\n",
    "lr = args.lr\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f'Epoch: {epoch:02}')\n",
    "    print('\\tTrain: ', end='')\n",
    "    train_loss, train_acc = train(\n",
    "        train_dl,\n",
    "        model,\n",
    "        criterion,\n",
    "        list(range(args.gpus)),\n",
    "        DEVICE,\n",
    "        optimizer,\n",
    "        scheduler=scheduler,\n",
    "        seq2seq=not classify,\n",
    "        pad_idx=vocab[CONFIG[\"pad_token\"]] if not classify else -1)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'lr' in param_group:\n",
    "            print(' | lr {:1.2E}'.format(param_group['lr']))\n",
    "            break\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    print('\\tValid: ', end='')\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = valid(\n",
    "            valid_dl,\n",
    "            model,\n",
    "            criterion,\n",
    "            DEVICE,\n",
    "            temperature=args.temp,\n",
    "            seq2seq=not classify,\n",
    "            pad_idx=vocab[CONFIG[\"pad_token\"]] if not classify else -1)\n",
    "    history['valid_loss'].append(valid_loss)\n",
    "    history['valid_acc'].append(valid_acc)\n",
    "\n",
    "    # Store best model so far\n",
    "    if valid_loss < best_val_loss:\n",
    "        best_model, best_val_loss = model, valid_loss\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model_to_save = best_model.module\\\n",
    "                if hasattr(best_model, 'module') else best_model\n",
    "            torch.save(model_to_save, model_name)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "#         # Additional Info when using cuda\n",
    "#         if DEVICE.type == 'cuda':\n",
    "#             print('Memory Usage:')\n",
    "#             for i in range(args.gpus):\n",
    "#                 max_alloc = round(\n",
    "#                     torch.cuda.max_memory_allocated(i) / 1024**3, 1)\n",
    "#                 cached = round(torch.cuda.memory_cached(i) / 1024**3, 1)\n",
    "#                 print(f'GPU: {i} Allocated: {max_alloc}G Cached: {cached}G')\n",
    "\n",
    "#         # if epoch > 10 and valid_loss > max(history['valid_loss'][-3:]):\n",
    "#         #     lr /= 2.\n",
    "#         #     for param_group in optimizer.param_groups:\n",
    "#         #         param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss,accuracy vs. time and save figures\n",
    "plot_training(history, CONFIG[\"SAVE_DIR\"], title=\"%s_lr%s\" % (args.model, args.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = DEVICE\n",
    "print(\"Evaluating predictions on test set\")\n",
    "# Load best model\n",
    "model = torch.load(model_name)\n",
    "if args.gpus:\n",
    "    model.to(device)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[CONFIG[\"begin_token\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bi_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, categorical, all_labs = [], [], []\n",
    "\n",
    "train_bi_preds = torch.zeros(len(train_ds), trg_y.shape[1], len(vocab)) \n",
    "valid_bi_preds = torch.zeros(len(valid_ds), trg_y.shape[1], len(vocab))\n",
    "\n",
    "# Calculate all predictions on test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    for enum, batch in enumerate(valid_dl):\n",
    "        \n",
    "        src = batch[0].to(device) \n",
    "        trg_y = batch[2].long().to(device)\n",
    "        trg_pos_mask= batch[3].to(device).squeeze() \n",
    "        trg_pad_mask = batch[4].to(device)\n",
    "        \n",
    "        memory = model.encode(src)\n",
    "        y = torch.zeros(src.size(0), 1, len(vocab)).long().to(device)\n",
    "        y[:, :, vocab[CONFIG[\"begin_token\"]]] = 1\n",
    "\n",
    "        bi_out = torch.zeros(len(batch[0]), trg_y.shape[1], len(vocab))\n",
    "        for i in range(trg_y.size(1)):\n",
    "            out = model.decode(memory, y,\n",
    "                               trg_pos_mask[:y.size(1), :y.size(1)],\n",
    "                               trg_pad_mask[:, :y.size(1)])[:, -1, :]\n",
    "            out = softmax(out / args.temp)\n",
    "            bi_out[:, i, :] = out\n",
    "            temp = torch.zeros(src.size(0), len(vocab)).long().to(device)\n",
    "            temp = temp.scatter_(1,\n",
    "                                 torch.argmax(out, dim=1).unsqueeze(-1), 1)\n",
    "            y = torch.cat([y, temp.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        y = y[:, 1:, :]\n",
    "        valid_bi_preds[enum*args.batch_size:(enum+1)*args.batch_size, :, :] = bi_out\n",
    "            \n",
    "        idx = (trg_y != vocab[CONFIG[\"pad_token\"]]).nonzero(as_tuple=True)\n",
    "        lab = trg_y[idx]\n",
    "        cat = torch.zeros((lab.size(0), len(vocab)),\n",
    "                          dtype=torch.long).to(lab.device)\n",
    "        cat = cat.scatter_(1, lab.unsqueeze(-1), 1)\n",
    "#         all_preds.extend(y[idx].cpu().numpy())\n",
    "#         categorical.extend(cat.cpu().numpy())\n",
    "#         all_labs.extend(lab.cpu().numpy())\n",
    "\n",
    "# all_preds = np.array(all_preds)\n",
    "# categorical = np.array(categorical)\n",
    "# all_labs = np.array(all_labs)\n",
    "# print(\"Calculated predictions\")\n",
    "\n",
    "# train_freq = train_ds.train_freq\n",
    "# if CONFIG[\"vocabulary\"] == 'spm':\n",
    "#     i2w = {i: vocab.IdToPiece(i) for i in range(len(vocab))}\n",
    "# markers = [\n",
    "#     CONFIG[\"begin_token\"], CONFIG[\"end_token\"], CONFIG[\"oov_token\"],\n",
    "#     CONFIG[\"pad_token\"]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bi_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_preds.shape)\n",
    "print(categorical.shape)\n",
    "print(all_labs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate top-k\n",
    "print(\"Evaluating top-k\")\n",
    "sys.stdout.flush()\n",
    "res = evaluate_topk(all_preds,\n",
    "                    all_labs,\n",
    "                    i2w,\n",
    "                    train_freq,\n",
    "                    CONFIG[\"SAVE_DIR\"],\n",
    "                    suffix='-val',\n",
    "                    min_train=args.vocab_min_freq,\n",
    "                    tokens_to_remove=markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ROC-AUC\n",
    "print(\"Evaluating ROC-AUC\")\n",
    "sys.stdout.flush()\n",
    "res.update(\n",
    "    evaluate_roc(all_preds,\n",
    "                 categorical,\n",
    "                 i2w,\n",
    "                 train_freq,\n",
    "                 CONFIG[\"SAVE_DIR\"],\n",
    "                 do_plot=not args.no_plot,\n",
    "                 min_train=args.vocab_min_freq,\n",
    "                 tokens_to_remove=markers))\n",
    "pprint(res.items())\n",
    "print(\"Saving results\")\n",
    "with open(CONFIG[\"SAVE_DIR\"] + \"results.json\", \"w\") as fp:\n",
    "    json.dump(res, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
