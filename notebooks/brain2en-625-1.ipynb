{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Seq2Seq (Neural Signals to Bi-grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for running a 2-word seq2seq Transformer where the neural signals are sent through the encoder while the corresponding bi-grams are sent through the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the seed for reproducibility. For more info read https://pytorch.org/docs/stable/notes/randomness.html and https://discuss.pytorch.org/t/random-seed-initialization/7854/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from transformers import AdamW\n",
    "\n",
    "from arg_parser import arg_parser\n",
    "from build_matrices import (build_design_matrices_classification,\n",
    "                            build_design_matrices_seq2seq)\n",
    "from config import build_config\n",
    "from dl_utils import Brain2enDataset, MyCollator\n",
    "from models import PITOM, ConvNet10, MeNTAL, MeNTALmini\n",
    "from train_eval import plot_training, train, valid\n",
    "from eval_utils import evaluate_roc, evaluate_topk\n",
    "from vocab_builder import get_sp_vocab, get_std_vocab, get_vocab\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = '20200531-ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 625\n",
      "Training Data:: Number of Conversations is: 63\n",
      "Validation Data:: Number of Conversations is: 13\n"
     ]
    }
   ],
   "source": [
    "args = arg_parser(['--subjects', '625',\n",
    "                   '--max-electrodes', '55',\n",
    "                   '--vocab-min-freq', '10',\n",
    "                   '--vocab-max-freq', '250',\n",
    "                  '--epochs', '50'])\n",
    "CONFIG = build_config(args, results_folder)\n",
    "args.gpus = min(args.gpus, torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model objectives\n",
    "MODEL_OBJ = {\n",
    "    \"ConvNet10\": \"classifier\",\n",
    "    \"PITOM\": \"classifier\",\n",
    "    \"MeNTALmini\": \"classifier\",\n",
    "    \"MeNTAL\": \"seq2seq\"\n",
    "}\n",
    "\n",
    "# GPUs\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "args.gpus = min(args.gpus, torch.cuda.device_count())\n",
    "\n",
    "# Fix random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "args.model = args.model.split(\"_\")[0]\n",
    "classify = False if (args.model in MODEL_OBJ\n",
    "                     and MODEL_OBJ[args.model] == \"seq2seq\") else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 625\n",
      "Training Data:: Number of Conversations is: 63\n",
      "Validation Data:: Number of Conversations is: 13\n"
     ]
    }
   ],
   "source": [
    "CONFIG = build_config(args, results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conversations: 63\n",
      "Vocabulary size (min_freq=10): 338\n",
      "Saving word counter\n"
     ]
    }
   ],
   "source": [
    "word2freq, word_list, n_classes, vocab, i2w = get_std_vocab(\n",
    "    CONFIG, comprehension=False, classify=classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some insights about the bigrams in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "Maximum Sequence Length (Preset): 60\n",
      "Number of train samples is: 4883\n",
      "Number of train labels is: 4883\n",
      "Maximum Sequence Length: 60\n",
      "Loading validation data\n",
      "Maximum Sequence Length (Preset): 60\n",
      "Number of valid samples is: 985\n",
      "Number of valid labels is: 985\n",
      "Maximum Sequence Length: 60\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading training data\")\n",
    "x_train, y_train = build_design_matrices_seq2seq(\n",
    "    'train', CONFIG, vocab, delimiter=\" \", aug_shift_ms=[-1000, -500], max_num_bins=60)\n",
    "\n",
    "# print(\"Loading validation data\")\n",
    "# x_valid, y_valid = build_design_matrices_seq2seq(\n",
    "#     'valid', CONFIG, vocab, delimiter=\" \", aug_shift_ms=[], max_num_bins=60, remove_unks=False)\n",
    "\n",
    "print(\"Loading validation data\")\n",
    "x_valid, y_valid = build_design_matrices_seq2seq(\n",
    "    'valid', CONFIG, vocab, delimiter=\" \", aug_shift_ms=[], max_num_bins=60, remove_unks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "269\n",
      "138\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "def replace_words(data):\n",
    "    df_y_train = pd.DataFrame(data)\n",
    "    df_y_train[1].replace(i2w, inplace=True)\n",
    "    df_y_train[2].replace(i2w, inplace=True)\n",
    "\n",
    "    return df_y_train\n",
    "\n",
    "\n",
    "def bigram_freq_excel(data, word2freq, i2w, filename, ref_data=None):\n",
    "    valid_df = replace_words(data)\n",
    "    valid_df = valid_df.groupby([1, 2]).size().reset_index(name='Count')\n",
    "    valid_df['BF1'] = valid_df[1].replace(dict(valid_df[1].value_counts()))\n",
    "    valid_df['BF2'] = valid_df[2].replace(dict(valid_df[2].value_counts()))\n",
    "    valid_df['VF1'] = valid_df[1].replace(word2freq)\n",
    "    valid_df['VF2'] = valid_df[2].replace(word2freq)\n",
    "\n",
    "    if ref_data is not None:\n",
    "        valid_df = valid_df.merge(ref_data, on=[1, 2], suffixes=('_valid', '_train'), how='left') \n",
    "        \n",
    "    valid_df.to_excel(os.path.join(CONFIG[\"SAVE_DIR\"], filename), index=False)\n",
    "        \n",
    "    print(len(valid_df[1].unique()))\n",
    "    print(len(valid_df[2].unique()))\n",
    "\n",
    "#     print(set(word2freq.keys()) - set(valid_df[1].unique()))\n",
    "#     print(set(word2freq.keys()) - set(valid_df[2].unique()))\n",
    "    \n",
    "    return valid_df\n",
    "\n",
    "\n",
    "train_df = bigram_freq_excel(y_train, word2freq, i2w, \"625_bi-gram-freq-train.xlsx\")\n",
    "valid_df = bigram_freq_excel(y_valid, word2freq, i2w, \"625_bi-gram-freq-valid.xlsx\", ref_data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>Count_valid</th>\n",
       "      <th>BF1_valid</th>\n",
       "      <th>BF2_valid</th>\n",
       "      <th>VF1_valid</th>\n",
       "      <th>VF2_valid</th>\n",
       "      <th>Count_train</th>\n",
       "      <th>BF1_train</th>\n",
       "      <th>BF2_train</th>\n",
       "      <th>VF1_train</th>\n",
       "      <th>VF2_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actually</td>\n",
       "      <td>feel</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actually</td>\n",
       "      <td>kinda</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>again</td>\n",
       "      <td>or</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ago</td>\n",
       "      <td>or</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all</td>\n",
       "      <td>or</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>you'll</td>\n",
       "      <td>come</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>you're</td>\n",
       "      <td>gonna</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>83</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>you're</td>\n",
       "      <td>not</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>180</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>you're</td>\n",
       "      <td>saying</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>your</td>\n",
       "      <td>at</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1       2  Count_valid  BF1_valid  BF2_valid  VF1_valid  \\\n",
       "0    actually    feel            1          2          1         50   \n",
       "1    actually   kinda            1          2          2         50   \n",
       "2       again      or            1          1          5         34   \n",
       "3         ago      or            1          1          5         23   \n",
       "4         all      or            1          1          5        116   \n",
       "..        ...     ...          ...        ...        ...        ...   \n",
       "242    you'll    come            1          1          3         12   \n",
       "243    you're   gonna            1          3          3         83   \n",
       "244    you're     not            1          3          7         83   \n",
       "245    you're  saying            2          3          1         83   \n",
       "246      your      at            1          1          7         52   \n",
       "\n",
       "     VF2_valid  Count_train  BF1_train  BF2_train  VF1_train  VF2_train  \n",
       "0           96          NaN        NaN        NaN        NaN        NaN  \n",
       "1           60          NaN        NaN        NaN        NaN        NaN  \n",
       "2           74          NaN        NaN        NaN        NaN        NaN  \n",
       "3           74          NaN        NaN        NaN        NaN        NaN  \n",
       "4           74          NaN        NaN        NaN        NaN        NaN  \n",
       "..         ...          ...        ...        ...        ...        ...  \n",
       "242         34          NaN        NaN        NaN        NaN        NaN  \n",
       "243         96          1.0        8.0        3.0       83.0       96.0  \n",
       "244        180          3.0        8.0       10.0       83.0      180.0  \n",
       "245         26          1.0        8.0        4.0       83.0       26.0  \n",
       "246         78          NaN        NaN        NaN        NaN        NaN  \n",
       "\n",
       "[247 rows x 12 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting train and validation data to Loader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 examples\n",
      "Number of training signals:  1343\n",
      "Skipped 0 examples\n",
      "Number of validation signals:  282\n"
     ]
    }
   ],
   "source": [
    "train_ds = Brain2enDataset(x_train, y_train)\n",
    "print(\"Number of training signals: \", len(train_ds))\n",
    "valid_ds = Brain2enDataset(x_valid, y_valid)\n",
    "print(\"Number of validation signals: \", len(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collator = MyCollator(CONFIG, vocab)\n",
    "train_dl = data.DataLoader(train_ds,\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=True,\n",
    "                           num_workers=CONFIG[\"num_cpus\"],\n",
    "                           collate_fn=my_collator)\n",
    "valid_dl = data.DataLoader(valid_ds,\n",
    "                           batch_size=args.batch_size,\n",
    "                           num_workers=CONFIG[\"num_cpus\"],\n",
    "                           collate_fn=my_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building default model: MeNTAL with 397714 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_MODELS = {\n",
    "    \"ConvNet10\": (len(vocab), ),\n",
    "    \"PITOM\": (len(vocab), sum(args.max_electrodes)),\n",
    "    \"MeNTALmini\":\n",
    "    (sum(args.max_electrodes), len(vocab), args.tf_dmodel, args.tf_nhead,\n",
    "     args.tf_nlayer, args.tf_dff, args.tf_dropout),\n",
    "    \"MeNTAL\": (sum(args.max_electrodes), len(vocab), args.tf_dmodel,\n",
    "               args.tf_nhead, args.tf_nlayer, args.tf_dff, args.tf_dropout)\n",
    "}\n",
    "\n",
    "# Create model\n",
    "if args.init_model is None:\n",
    "    if args.model in DEFAULT_MODELS:\n",
    "        print(\"Building default model: %s\" % args.model, end=\"\")\n",
    "        model_class = globals()[args.model]\n",
    "        model = model_class(*(DEFAULT_MODELS[args.model]))\n",
    "    else:\n",
    "        print(\"Building custom model: %s\" % args.model, end=\"\")\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    model_name = \"%s%s.pt\" % (SAVE_DIR, args.model)\n",
    "    if os.path.isfile(model_name):\n",
    "        model = torch.load(model_name)\n",
    "        model = model.module if hasattr(model, 'module') else model\n",
    "        print(\"Loaded initial model: %s \" % args.model)\n",
    "    else:\n",
    "        print(\"No models found in: \", SAVE_DIR)\n",
    "        sys.exit(1)\n",
    "print(\" with %d trainable parameters\" %\n",
    "      sum([p.numel() for p in model.parameters() if p.requires_grad]))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "step_size = int(math.ceil(len(train_ds) / args.batch_size))\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=args.lr,\n",
    "                  weight_decay=args.weight_decay)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 0 GPU(s) with batch_size 48 for 5 epochs\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on %d GPU(s) with batch_size %d for %d epochs\" %\n",
    "      (args.gpus, args.batch_size, args.epochs))\n",
    "print(\"=\" * CONFIG[\"print_pad\"])\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = model\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'valid_loss': [],\n",
    "    'valid_acc': []\n",
    "}\n",
    "\"\"\" train_loss_compute = SimpleLossCompute(criterion,\n",
    "                                       opt=optimizer,\n",
    "                                       scheduler=scheduler)\n",
    "valid_loss_compute = SimpleLossCompute(criterion, opt=None, scheduler=None)\n",
    "\"\"\"\n",
    "epoch = 0\n",
    "model_name = \"%s%s.pt\" % (CONFIG[\"SAVE_DIR\"], args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 0 GPU(s) with batch_size 48 for 5 epochs\n",
      "Epoch: 01\n",
      "\tTrain: loss 5.111 | perplexity 165.88 | ms/batch 490.21 | lr 1.00E-04\n",
      "\tValid: loss 4.674 | perplexity 107.13\n",
      "Epoch: 02\n",
      "\tTrain: loss 4.589 | perplexity 98.38 | ms/batch 502.43 | lr 1.00E-04\n",
      "\tValid: loss 4.525 | perplexity 92.26\n",
      "Epoch: 03\n",
      "\tTrain: loss 4.465 | perplexity 86.90 | ms/batch 368.25 | lr 1.00E-04\n",
      "\tValid: loss 4.456 | perplexity 86.17\n",
      "Epoch: 04\n",
      "\tTrain: loss 4.392 | perplexity 80.78 | ms/batch 498.86 | lr 1.00E-04\n",
      "\tValid: loss 4.406 | perplexity 81.95\n",
      "Epoch: 05\n",
      "\tTrain: loss 4.330 | perplexity 75.94 | ms/batch 165.94 | lr 1.00E-04\n",
      "\tValid: loss 4.362 | perplexity 78.44\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining on %d GPU(s) with batch_size %d for %d epochs\" %\n",
    "      (args.gpus, args.batch_size, args.epochs))\n",
    "sys.stdout.flush()\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model = model\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'valid_loss': [],\n",
    "    'valid_acc': []\n",
    "}\n",
    "\"\"\" train_loss_compute = SimpleLossCompute(criterion,\n",
    "                                       opt=optimizer,\n",
    "                                       scheduler=scheduler)\n",
    "valid_loss_compute = SimpleLossCompute(criterion, opt=None, scheduler=None)\n",
    "\"\"\"\n",
    "epoch = 0\n",
    "model_name = \"%s%s.pt\" % (CONFIG[\"SAVE_DIR\"], args.model)\n",
    "\"\"\" totalfreq = float(sum(train_ds.train_freq.values()))\n",
    "print(\n",
    "    sorted(((i2w[l], f / totalfreq)\n",
    "            for l, f in train_ds.train_freq.most_common()),\n",
    "           key=lambda x: -x[1]))\n",
    "\"\"\"\n",
    "# Run training and validation for args.epochs epochs\n",
    "lr = args.lr\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f'Epoch: {epoch:02}')\n",
    "    print('\\tTrain: ', end='')\n",
    "    train_loss, train_acc = train(\n",
    "        train_dl,\n",
    "        model,\n",
    "        criterion,\n",
    "        list(range(args.gpus)),\n",
    "        DEVICE,\n",
    "        optimizer,\n",
    "        scheduler=scheduler,\n",
    "        seq2seq=not classify,\n",
    "        pad_idx=vocab[CONFIG[\"pad_token\"]] if not classify else -1)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'lr' in param_group:\n",
    "            print(' | lr {:1.2E}'.format(param_group['lr']))\n",
    "            break\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    print('\\tValid: ', end='')\n",
    "    with torch.no_grad():\n",
    "        valid_loss, valid_acc = valid(\n",
    "            valid_dl,\n",
    "            model,\n",
    "            criterion,\n",
    "            DEVICE,\n",
    "            temperature=args.temp,\n",
    "            seq2seq=not classify,\n",
    "            pad_idx=vocab[CONFIG[\"pad_token\"]] if not classify else -1)\n",
    "    history['valid_loss'].append(valid_loss)\n",
    "    history['valid_acc'].append(valid_acc)\n",
    "\n",
    "    # Store best model so far\n",
    "    if valid_loss < best_val_loss:\n",
    "        best_model, best_val_loss = model, valid_loss\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model_to_save = best_model.module\\\n",
    "                if hasattr(best_model, 'module') else best_model\n",
    "            torch.save(model_to_save, model_name)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "#         # Additional Info when using cuda\n",
    "#         if DEVICE.type == 'cuda':\n",
    "#             print('Memory Usage:')\n",
    "#             for i in range(args.gpus):\n",
    "#                 max_alloc = round(\n",
    "#                     torch.cuda.max_memory_allocated(i) / 1024**3, 1)\n",
    "#                 cached = round(torch.cuda.memory_cached(i) / 1024**3, 1)\n",
    "#                 print(f'GPU: {i} Allocated: {max_alloc}G Cached: {cached}G')\n",
    "\n",
    "#         # if epoch > 10 and valid_loss > max(history['valid_loss'][-3:]):\n",
    "#         #     lr /= 2.\n",
    "#         #     for param_group in optimizer.param_groups:\n",
    "#         #         param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating predictions on test set\n"
     ]
    }
   ],
   "source": [
    "device = DEVICE\n",
    "print(\"Evaluating predictions on test set\")\n",
    "# Load best model\n",
    "best_model = torch.load(model_name)\n",
    "if args.gpus:\n",
    "    best_model.to(device)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_neural_signal(model, data_iterator, data_set_len, vocab_len):\n",
    "\n",
    "    valid_bi_preds = torch.zeros(data_set_len, 3, vocab_len)\n",
    "    all_trg_y = torch.zeros(data_set_len, 3, dtype=torch.int32)\n",
    "\n",
    "    # Calculate all predictions on test set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for enum, batch in enumerate(data_iterator):\n",
    "\n",
    "            src = batch[0].to(device) \n",
    "            trg_y = batch[2].long().to(device)\n",
    "            trg_pos_mask= batch[3].to(device).squeeze() \n",
    "            trg_pad_mask = batch[4].to(device)\n",
    "\n",
    "            all_trg_y[enum*args.batch_size:(enum+1)*args.batch_size, :] = trg_y\n",
    "\n",
    "            memory = model.encode(src)\n",
    "            y = torch.zeros(src.size(0), 1, len(vocab)).long().to(device)\n",
    "            y[:, :, vocab[CONFIG[\"begin_token\"]]] = 1\n",
    "\n",
    "            bi_out = torch.zeros(len(batch[0]), trg_y.shape[1], len(vocab))\n",
    "            for i in range(trg_y.size(1)):\n",
    "                out = model.decode(memory, y,\n",
    "                                   trg_pos_mask[:y.size(1), :y.size(1)],\n",
    "                                   trg_pad_mask[:, :y.size(1)])[:, -1, :]\n",
    "                out = softmax(out / args.temp)\n",
    "                bi_out[:, i, :] = out\n",
    "                temp = torch.zeros(src.size(0), vocab_len).long().to(device)\n",
    "                temp = temp.scatter_(1,\n",
    "                                     torch.argmax(out, dim=1).unsqueeze(-1), 1)\n",
    "                y = torch.cat([y, temp.unsqueeze(1)], dim=1)\n",
    "            valid_bi_preds[enum*args.batch_size:(enum+1)*args.batch_size, :, :] = bi_out\n",
    "\n",
    "        topk_preds = torch.topk(valid_bi_preds, 10).indices\n",
    "        topk_preds = topk_preds.view(data_set_len, -1)\n",
    "        all_preds = valid_bi_preds.view(data_set_len, -1)\n",
    "\n",
    "    return all_trg_y, topk_preds, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_ds)\n",
    "valid_size = len(valid_ds)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "valid_all_trg_y, valid_topk_preds, valid_all_preds = translate_neural_signal(best_model, valid_dl, valid_size, vocab_len)\n",
    "train_all_trg_y, train_topk_preds, train_all_preds = translate_neural_signal(best_model, train_dl, train_size, vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rank(x, string):\n",
    "    word = x[string]\n",
    "    string = string + '_0*'\n",
    "    word_preds = x.filter(regex=string)\n",
    "    \n",
    "    try:\n",
    "        rank = np.where(word == word_preds)[0][0]\n",
    "    except IndexError:\n",
    "        rank = pd.NA\n",
    "    \n",
    "    return rank\n",
    "\n",
    "\n",
    "def fill_topk_cols(x, string):\n",
    "    rank = x['_'.join([string, 'rank'])]\n",
    "    \n",
    "    if pd.isna(rank):\n",
    "        abc = [0, 0, 0]\n",
    "    elif rank == 0:\n",
    "        abc = [1, 0, 0]\n",
    "    elif rank < 5:\n",
    "        abc = [0, 1, 0]\n",
    "    elif rank < 10:\n",
    "        abc =  [0, 0, 1]\n",
    "    else:\n",
    "        abc = [0, 0, 0]\n",
    "\n",
    "    return abc\n",
    "       \n",
    "\n",
    "def create_excel_preds(targets, top_predictions, i2w):\n",
    "    df = pd.DataFrame(targets.numpy(), columns = ['word1', 'word2', 'word3'])\n",
    "    df = df.drop(columns = ['word3'])\n",
    "    pred_col_names = ['_'.join([word, str(i).zfill(2)]) for word in ['word1', 'word2']\n",
    "                      for i in range(1, 11)]\n",
    "    df[pred_col_names] = pd.DataFrame(top_predictions.numpy()[:, :20])\n",
    "    top_col_names = ['_'.join([word, 't' + str(i)]) for word in ['word1', 'word2']\n",
    "                    for i in [1, 5, 10]]\n",
    "    \n",
    "    df['word1_rank'] = df.apply(calc_rank, axis=1, args=('word1',))\n",
    "    df['word2_rank'] = df.apply(calc_rank, axis=1, args=('word2',))\n",
    "    \n",
    "    df[['word1_top1', 'word1_top5', 'word1_top10']] = pd.DataFrame(df.apply(fill_topk_cols, axis=1, args=('word1', )).tolist())\n",
    "    df[['word2_top1', 'word2_top5', 'word2_top10']] = pd.DataFrame(df.apply(fill_topk_cols, axis=1, args=('word2', )).tolist())\n",
    "\n",
    "    df[pred_col_names] = df[pred_col_names].replace(i2w)\n",
    "    df['word1'] = df['word1'].replace(i2w)\n",
    "    df['word2'] = df['word2'].replace(i2w)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds_df = create_excel_preds(valid_all_trg_y, valid_topk_preds, i2w)\n",
    "train_preds_df = create_excel_preds(train_all_trg_y, train_topk_preds, i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds_df.to_excel(os.path.join(CONFIG[\"SAVE_DIR\"], 'valid_set.xlsx'), index=False)\n",
    "train_preds_df.to_excel(os.path.join(CONFIG[\"SAVE_DIR\"], 'train_set.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       35\n",
       "1      197\n",
       "2      322\n",
       "3      173\n",
       "4      204\n",
       "      ... \n",
       "277    294\n",
       "278    197\n",
       "279    181\n",
       "280    171\n",
       "281    218\n",
       "Name: word1, Length: 282, dtype: int32"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds_df['word1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0010, 0.2417, 0.0017,  ..., 0.0020, 0.0026, 0.0009],\n",
       "        [0.0011, 0.2523, 0.0016,  ..., 0.0018, 0.0025, 0.0011],\n",
       "        [0.0011, 0.2459, 0.0017,  ..., 0.0021, 0.0024, 0.0011],\n",
       "        ...,\n",
       "        [0.0011, 0.2586, 0.0016,  ..., 0.0018, 0.0024, 0.0010],\n",
       "        [0.0011, 0.2667, 0.0015,  ..., 0.0019, 0.0024, 0.0010],\n",
       "        [0.0012, 0.2788, 0.0014,  ..., 0.0018, 0.0022, 0.0012]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_all_preds[:, 0:338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def get_roc_auc_ovo_scores(data, all_preds, string, i2w):\n",
    "    y_test = data['word1']\n",
    "    vocab_len = len(i2w)\n",
    "    \n",
    "    if string == 'word1':\n",
    "        softmax_range = list(range(0, vocab_len))\n",
    "    elif string == 'word2':\n",
    "        softmax_range = list(range(len(i2w), 2 * len(i2w)))\n",
    "        \n",
    "    y_prob = all_preds[:, softmax_range]\n",
    "    \n",
    "    labels = list(i2w.keys())\n",
    "    \n",
    "    macro_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=\"ovo\",\n",
    "                                  average=\"macro\", labels=labels)\n",
    "    \n",
    "    weighted_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=\"ovo\",\n",
    "                                     average=\"weighted\", labels=labels)\n",
    "    print(\"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "      \"(weighted by prevalence)\"\n",
    "      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))\n",
    "\n",
    "\n",
    "\n",
    "def get_roc_auc_ovr_scores(data, all_preds, string, i2w):\n",
    "    y_test = data['word1']\n",
    "    vocab_len = len(i2w)\n",
    "    \n",
    "    if string == 'word1':\n",
    "        softmax_range = list(range(0, vocab_len))\n",
    "    elif string == 'word2':\n",
    "        softmax_range = list(range(len(i2w), 2 * len(i2w)))\n",
    "        \n",
    "    y_prob = all_preds[:, softmax_range]\n",
    "    \n",
    "    labels = list(i2w.keys())\n",
    "    \n",
    "    macro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n",
    "                                  average=\"micro\", labels=labels)\n",
    "    weighted_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n",
    "                                     average=\"weighted\", labels=labels)\n",
    "    print(\"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "      \"(weighted by prevalence)\"\n",
    "      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-One ROC AUC scores:\n",
      "0.552442 (macro),\n",
      "0.545307 (weighted by prevalence)\n",
      "One-vs-One ROC AUC scores:\n",
      "0.552245 (macro),\n",
      "0.544587 (weighted by prevalence)\n",
      "One-vs-One ROC AUC scores:\n",
      "0.538355 (macro),\n",
      "0.529472 (weighted by prevalence)\n",
      "One-vs-One ROC AUC scores:\n",
      "0.541047 (macro),\n",
      "0.530183 (weighted by prevalence)\n"
     ]
    }
   ],
   "source": [
    "get_roc_auc_ovo_scores(train_preds_df, train_all_preds, 'word1', i2w)\n",
    "get_roc_auc_ovo_scores(train_preds_df, train_all_preds, 'word2', i2w)\n",
    "get_roc_auc_ovo_scores(valid_preds_df, valid_all_preds, 'word1', i2w)\n",
    "get_roc_auc_ovo_scores(valid_preds_df, valid_all_preds, 'word2', i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-4612e053e82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_roc_auc_ovr_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_preds_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_all_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_roc_auc_ovr_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_preds_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_all_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_roc_auc_ovr_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_preds_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_all_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_roc_auc_ovr_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_preds_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_all_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-5045783650d6>\u001b[0m in \u001b[0;36mget_roc_auc_ovr_scores\u001b[0;34m(data, all_preds, string, i2w)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#     macro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#                                   average=\"macro\", labels=labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     weighted_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\",\n\u001b[0m\u001b[1;32m     49\u001b[0m                                      average=\"weighted\", labels=labels)\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#     print(\"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch-env/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[0m\u001b[1;32m    383\u001b[0m                                          multi_class, average, sample_weight)\n\u001b[1;32m    384\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch-env/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_multiclass_roc_auc_score\u001b[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# ovr is same as multi-label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0my_true_multilabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         return _average_binary_score(_binary_roc_auc_score, y_true_multilabel,\n\u001b[0m\u001b[1;32m    494\u001b[0m                                      \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                                      sample_weight=sample_weight)\n",
      "\u001b[0;32m~/.conda/envs/torch-env/lib/python3.8/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0my_true_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnot_average_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0my_score_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnot_average_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         score[c] = binary_metric(y_true_c, y_score_c,\n\u001b[0m\u001b[1;32m    120\u001b[0m                                  sample_weight=score_weight)\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch-env/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;34m\"\"\"Binary roc auc score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    222\u001b[0m                          \"is not defined in that case.\")\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "get_roc_auc_ovr_scores(train_preds_df, train_all_preds, 'word1', i2w)\n",
    "get_roc_auc_ovr_scores(train_preds_df, train_all_preds, 'word2', i2w)\n",
    "get_roc_auc_ovr_scores(valid_preds_df, valid_all_preds, 'word1', i2w)\n",
    "get_roc_auc_ovr_scores(valid_preds_df, valid_all_preds, 'word2', i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "def get_confusion_matrix(data, string, i2w):\n",
    "    y_true = data[string]\n",
    "    y_pred = data['_'.join([string, '01'])]\n",
    "    labels = list(i2w.keys())\n",
    "    data_cm = confusion_matrix(valid_preds_df['word2'],\n",
    "                                          valid_preds_df['word2_01'],\n",
    "                                          labels=labels)\n",
    "    df_cm = pd.DataFrame(data_cm, labels, labels)\n",
    "    return data_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_w1_cm = plot_confusion_matrix(valid_preds_df, 'word1', i2w)\n",
    "valid_w2_cm = plot_confusion_matrix(valid_preds_df, 'word2', i2w)\n",
    "train_w1_cm = plot_confusion_matrix(train_preds_df, 'word1', i2w)\n",
    "train_w2_cm = plot_confusion_matrix(train_preds_df, 'word2', i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate top-k\n",
    "print(\"Evaluating top-k\")\n",
    "sys.stdout.flush()\n",
    "res = evaluate_topk(all_preds,\n",
    "                    all_labs,\n",
    "                    i2w,\n",
    "                    train_freq,\n",
    "                    CONFIG[\"SAVE_DIR\"],\n",
    "                    suffix='-val',\n",
    "                    min_train=args.vocab_min_freq,\n",
    "                    tokens_to_remove=markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ROC-AUC\n",
    "print(\"Evaluating ROC-AUC\")\n",
    "sys.stdout.flush()\n",
    "res.update(\n",
    "    evaluate_roc(all_preds,\n",
    "                 categorical,\n",
    "                 i2w,\n",
    "                 train_freq,\n",
    "                 CONFIG[\"SAVE_DIR\"],\n",
    "                 do_plot=not args.no_plot,\n",
    "                 min_train=args.vocab_min_freq,\n",
    "                 tokens_to_remove=markers))\n",
    "pprint(res.items())\n",
    "print(\"Saving results\")\n",
    "with open(CONFIG[\"SAVE_DIR\"] + \"results.json\", \"w\") as fp:\n",
    "    json.dump(res, fp, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
