{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time:  Friday 15/05/2020 00:47:57\n",
      "Subject: 625\n",
      "Training Data:: Number of Conversations is: 63\n",
      "Validation Data:: Number of Conversations is: 13\n",
      "Subject: 676\n",
      "Training Data:: Number of Conversations is: 49\n",
      "Validation Data:: Number of Conversations is: 24\n",
      "Building vocabulary\n",
      "# Conversations: 112\n",
      "Vocabulary size (min_freq=10): 1393\n",
      "Saving word counter\n",
      "Loading training data\n",
      "NY625_418_Part3_conversation1\n",
      "Number of train samples is: 2777\n",
      "Number of train samples is: 2777\n",
      "Maximum Sequence Length is: 1028\n",
      "Loading validation data\n",
      "NY625_421_Part4_one_conversation2\n",
      "Number of valid samples is: 699\n",
      "Number of valid samples is: 699\n",
      "Maximum Sequence Length is: 3799\n",
      "Skipped 5 examples\n",
      "Number of training signals:  2772\n",
      "Skipped 5 examples\n",
      "Number of validation signals:  694\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from transformers import AdamW\n",
    "\n",
    "from arg_parser import arg_parser\n",
    "from build_matrices import (build_design_matrices_classification,\n",
    "                            build_design_matrices_seq2seq)\n",
    "from config import build_config\n",
    "from dl_utils import Brain2enDataset, MyCollator\n",
    "from models import PITOM, ConvNet10, MeNTAL, MeNTALmini\n",
    "from train_eval import evaluate_roc, evaluate_topk, plot_training, train, valid\n",
    "from vocab_builder import get_sp_vocab, get_std_vocab, get_vocab\n",
    "\n",
    "# from train_eval import *\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%A %d/%m/%Y %H:%M:%S\")\n",
    "print(\"Start Time: \", dt_string)\n",
    "results_str = now.strftime(\"%Y-%m-%d-%H:%M\")\n",
    "\n",
    "args = arg_parser()\n",
    "CONFIG = build_config(args, results_str)\n",
    "\n",
    "# sys.stdout = open(CONFIG[\"LOG_FILE\"], 'w')\n",
    "\n",
    "# Model objectives\n",
    "MODEL_OBJ = {\n",
    "    \"ConvNet10\": \"classifier\",\n",
    "    \"PITOM\": \"classifier\",\n",
    "    \"MeNTALmini\": \"classifier\",\n",
    "    \"MeNTAL\": \"seq2seq\"\n",
    "}\n",
    "\n",
    "# GPUs\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "args.gpus = min(args.gpus, torch.cuda.device_count())\n",
    "\n",
    "# Fix random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "args.model = args.model.split(\"_\")[0]\n",
    "classify = False if (args.model in MODEL_OBJ\n",
    "                     and MODEL_OBJ[args.model] == \"seq2seq\") else True\n",
    "\n",
    "CONV_DIRS = CONFIG[\"CONV_DIRS\"]\n",
    "SAVE_DIR = CONFIG[\"SAVE_DIR\"]\n",
    "TRAIN_CONV = CONFIG[\"TRAIN_CONV\"]\n",
    "VALID_CONV = CONFIG[\"VALID_CONV\"]\n",
    "\n",
    "# Load train and validation datasets\n",
    "# (if model is seq2seq, using speaker switching for sentence cutoff,\n",
    "# and custom batching)\n",
    "if classify:\n",
    "    print(\"Building vocabulary\")\n",
    "    word2freq, vocab, n_classes, w2i, i2w = get_vocab(CONFIG)\n",
    "\n",
    "    print(\"Loading training data\")\n",
    "    x_train, y_train = build_design_matrices_classification(\n",
    "        'train', CONFIG, w2i, delimiter=\" \", aug_shift_ms=[-1000])\n",
    "    sys.stdout.flush()\n",
    "    print(\"Loading validation data\")\n",
    "    x_valid, y_valid = build_design_matrices_classification('valid',\n",
    "                                                            CONFIG,\n",
    "                                                            w2i,\n",
    "                                                            delimiter=\" \",\n",
    "                                                            aug_shift_ms=[])\n",
    "    sys.stdout.flush()\n",
    "    if args.model == \"ConvNet10\":\n",
    "        x_train = x_train[:, np.newaxis, ...]\n",
    "        x_valid = x_valid[:, np.newaxis, ...]\n",
    "\n",
    "    # Shuffle labels if required\n",
    "    if args.shuffle:\n",
    "        print(\"Shuffling labels\")\n",
    "        np.random.shuffle(y_train)\n",
    "        np.random.shuffle(y_valid)\n",
    "\n",
    "    x_train = torch.from_numpy(x_train).float()\n",
    "    print(\"Shape of training signals: \", x_train.size())\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    train_ds = data.TensorDataset(x_train, y_train)\n",
    "\n",
    "    x_valid = torch.from_numpy(x_valid).float()\n",
    "    print(\"Shape of validation signals: \", x_valid.size())\n",
    "    y_valid = torch.from_numpy(y_valid)\n",
    "    valid_ds = data.TensorDataset(x_valid, y_valid)\n",
    "\n",
    "    # Create dataset and data generators\n",
    "    print(\"Creating dataset and generators\")\n",
    "    sys.stdout.flush()\n",
    "    train_dl = data.DataLoader(train_ds,\n",
    "                               batch_size=args.batch_size,\n",
    "                               shuffle=True,\n",
    "                               num_workers=CONFIG[\"num_cpus\"])\n",
    "    valid_dl = data.DataLoader(valid_ds,\n",
    "                               batch_size=args.batch_size,\n",
    "                               num_workers=CONFIG[\"num_cpus\"])\n",
    "else:\n",
    "    print(\"Building vocabulary\")\n",
    "    if CONFIG[\"vocabulary\"] == 'spm':\n",
    "        vocab = get_sp_vocab(CONFIG, algo='unigram', vocab_size=500)\n",
    "    elif CONFIG[\"vocabulary\"] == 'std':\n",
    "        word2freq, word_list, n_classes, vocab, i2w = get_std_vocab(\n",
    "            CONFIG, classify)\n",
    "    else:\n",
    "        print(\"Such vocabulary doesn't exist\")\n",
    "    # print([(i, vocab.IdToPiece(i)) for i in range(len(vocab))])\n",
    "\n",
    "    print(\"Loading training data\")\n",
    "    x_train, y_train = build_design_matrices_seq2seq(\n",
    "        'train', CONFIG, vocab, delimiter=\" \", aug_shift_ms=[-1000, -500])\n",
    "\n",
    "    print(\"Loading validation data\")\n",
    "    x_valid, y_valid = build_design_matrices_seq2seq('valid',\n",
    "                                                     CONFIG,\n",
    "                                                     vocab,\n",
    "                                                     delimiter=\" \",\n",
    "                                                     aug_shift_ms=[])\n",
    "    sys.stdout.flush()\n",
    "    # Shuffle labels if required\n",
    "    if args.shuffle:\n",
    "        print(\"Shuffling labels\")\n",
    "        np.random.shuffle(y_train)\n",
    "        np.random.shuffle(y_valid)\n",
    "    train_ds = Brain2enDataset(x_train, y_train)\n",
    "    print(\"Number of training signals: \", len(train_ds))\n",
    "    valid_ds = Brain2enDataset(x_valid, y_valid)\n",
    "    print(\"Number of validation signals: \", len(valid_ds))\n",
    "    my_collator = MyCollator(CONFIG, vocab)\n",
    "    train_dl = data.DataLoader(train_ds,\n",
    "                               batch_size=args.batch_size,\n",
    "                               shuffle=True,\n",
    "                               num_workers=CONFIG[\"num_cpus\"],\n",
    "                               collate_fn=my_collator)\n",
    "    valid_dl = data.DataLoader(valid_ds,\n",
    "                               batch_size=args.batch_size,\n",
    "                               num_workers=CONFIG[\"num_cpus\"],\n",
    "                               collate_fn=my_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, trg, trg_y, pos_mask, pad_mask = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 60, 128])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128]),\n",
       " torch.Size([48, 128])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_ds[i][0].shape for i in range(48*21, 48*22)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "src = pad_sequence([train_ds[i][0] for i in range(48, 96)],\n",
    "                           batch_first=True,\n",
    "                           padding_value=0.)\n",
    "labels = pad_sequence([train_ds[i][1] for i in range(48, 96)],\n",
    "                              batch_first=True,\n",
    "                              padding_value=vocab['<pad>'])\n",
    "trg = torch.zeros(labels.size(0), labels.size(1),\n",
    "                          len(vocab)).scatter_(\n",
    "                              2, labels.unsqueeze(-1), 1)\n",
    "trg, trg_y = trg[:, :-1, :], labels[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks(labels):\n",
    "        pos_mask = (torch.triu(torch.ones(labels.size(1),\n",
    "                                          labels.size(1))) == 1).transpose(\n",
    "                                              0, 1).unsqueeze(0)\n",
    "        pos_mask = pos_mask.float().masked_fill(pos_mask == 0,\n",
    "                                                float('-inf')).masked_fill(\n",
    "                                                    pos_mask == 1, float(0.0))\n",
    "        pad_mask = labels == vocab['<pad>']\n",
    "        \n",
    "        return pos_mask, pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
